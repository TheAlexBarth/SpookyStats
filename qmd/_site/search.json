[
  {
    "objectID": "ex01_single-parameter.html",
    "href": "ex01_single-parameter.html",
    "title": "What do we know about monsters?",
    "section": "",
    "text": "Zombie bites - Estimating a binomial probability\nUh oh! You just got bit by a zombie. What’s the probability that you will now become a zombie too? Everyone knows that zombies are pretty infectious but maybe you were able to skirt by…\nLet’s simuate data for a vector of zombie bites. Let’s say that if a zombie bites you, there’s a 75% probability you’d get infected. We can simulate 25 events of zombie bites to observe our data from:\n\nData SimulationModel SpecificationStan CodeAnalysisFrequentist Comparison\n\n\n\n###\n# Single parameter examples\n###\nset.seed(1031)\nlibrary(ggplot2)\nlibrary(bayesplot)\nlibrary(rstan)\n\n###\n# First for a binomial value ######\n###\n\n\n# \\- simulate the data ---------\np = 0.75\nnum_cases = 50\nbites = rbinom(num_cases,1,p)\n\nIt’s worth noting that our simulated data shows a different simulated value than our true probability - which will influence our posterior estimates!\n\n\nWe have to specify our model to be able to construct it into stan code. We are going to use a bernoulli distribution as our data model (we think that zombie infection following a bite will occur with some probability (\\(\\theta\\)) according to a bernoulli distirubiton). Note this also could be done with the binomial distribution…\nWe have to then model our prior belief about the infection probability. Since we simluated the data, we actually do know the true parameter value, but we pretend to learn. So let’s say you are pretty optimisitic and think it may only be a 50/50 chance that infection occurs after a bite, but you’re not too sure. We can then model our prior distribution as a beta(2,2), which is loosely centered around 0.5, but not too strong in it.\nSo our model looks like this:\n\\[\n\\begin{align}\nData \\ Model \\ &\\{ \\ y_i \\sim bernoulli(\\theta) \\\\\nParameter \\ Model \\ &\\{ \\theta \\sim beta(2,2)\n\\end{align}\n\\]\n\n\n\ndata {\n  int&lt;lower=0&gt; n;         // Number of trials\n  array[n] int&lt;lower=0, upper=1&gt; y; // Observed outcomes (0 or 1)\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p; // Probability of success\n}\n\nmodel {\n    // Prior:\n    p ~ beta(2, 2);\n    \n    y ~ bernoulli(p);\n}\n\n\n\nWe call the stan code in R to run the model:\n\n# \\- Set up data for stan\n\nbite_data &lt;- list(\n  n = num_cases,\n  y = bites\n)\n\nzombie_fit &lt;- stan('./stan/ex01_single-param.stan', 'zombie-mod', data = bite_data, iter = 3000, warmup = 1000)\n\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nInference for Stan model: zombie-mod.\n4 chains, each with iter=3000; warmup=1000; thin=1; \npost-warmup draws per chain=2000, total post-warmup draws=8000.\n\n       mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\np      0.66    0.00 0.06   0.52   0.61   0.66   0.70   0.78  2809    1\nlp__ -33.29    0.01 0.67 -35.19 -33.46 -33.03 -32.85 -32.80  3952    1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct 30 23:54:43 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe text which follows our model output gives us a lot of useful information both to the posterior values and model convergence diagnostics.\nWe can also visualize the distribution estimated by the posterior value. I’ve added a vertical red line for the true value, but in most real cases, you don’t know the “true” value.\n\n# get posterior for the estimates of p\nzombie_p_post &lt;- rstan::extract(zombie_fit)$p\n\nposterior_param_plot(zombie_p_post, dbeta, seq(0,1,length.out = 1000), p, shape1 = 2, shape2 = 2)\n\n\n\n\n\n\n\n\nThe posterior high-density interval captures the true value. However, it is worth noting that our data was simulated, and did not reflect the true value that closely (0.66 versus 0.75)\n\n\nFor fun, I’ll add a frequentist comparison:\n\nprop.test(x = sum(bites), n = length(bites))\n\n\n    1-sample proportions test with continuity correction\n\ndata:  sum(bites) out of length(bites), null probability 0.5\nX-squared = 4.5, df = 1, p-value = 0.03389\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5114459 0.7840536\nsample estimates:\n   p \n0.66 \n\n\nWe could use this approach to say: “we are 95% confident that the true value is between 0.51 and 0.78.”. Or you could talk about the p-value and say our observed propotion of bite cases is significantly different from 0.5.\nSo in this case, it is not really that different than our Bayesian analysis, just a philosophical difference in the way we interpret the underlying process.\n\n\n\n\n\nWerewolf weights - Estimating a mean value\nI’ve googled around and found that werewolfs are up to 180kg. Let’s simulate some data to reflect this.\n\nData SimulationModel StructureStanAnalysisModel Checking\n\n\nGiven the fact they are up to 180kg, let’s assume they are average 140 with a standard deviation of 15kg - a pretty big range!\n\nmu &lt;- 150\nsigma &lt;- 15\n\nwerewolfs &lt;- 50\nobs_wereweight &lt;- rnorm(werewolfs, mu, sigma)\n\nggplot() +\n  geom_histogram(aes(x = obs_wereweight)) +\n  labs(x = 'Werewolf Weight (kg)') +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nThe overall idea here is similar, but now we are estimating both the variation and mean value\n\\[\n\\begin{align}\ny &\\sim \\mathcal{N}(\\mu, \\sigma^2)\\\\\n\\\\\n\\mu &\\sim \\mathcal{N}(150, 40) \\\\\n\\sigma &\\sim \\mathcal{N}(30,10)\n\\end{align}\n\\]\n\n\n\ndata {\n  int&lt;lower=0&gt; n;            // Number of observations\n  array[n] real y;                 // Observed data\n}\n\nparameters {\n  real mu;\n  real sigma;\n}\n\nmodel {\n    sigma ~ normal(30, 10);\n    mu ~ normal(150, 30);\n    \n    y ~ normal(mu, sigma);\n}\n\n\n\n\nwere_stan_data &lt;- list(\n  n = werewolfs,\n  y = obs_wereweight\n)\n\nwere_fit &lt;- stan(\n  './stan/ex01_wereweight.stan', 'wereweight', \n  data = were_stan_data, iter = 3000, warmup = 500\n)\n\nposterior_wolfs &lt;- extract(were_fit) \nposterior_param_plot(\n  posterior_wolfs$mu, dnorm, \n  seq(100, 190, length.out = 10000), mu,\n  mean = 150, sd = 30\n)\nposterior_param_plot(\n  posterior_wolfs$sigma, dnorm, \n  seq(5, 40, length.out = 10000), sigma, mean = 30, sd = 10\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo we can see\n\n\nI’m not going to go through a whole model checking feature for every example. However, I’ll again show right here the basic things to look at:\n\nDoes it match our observations? Yes, not super exciting with simulated data but something worth considering\nDid the model converge?\n\nFirst we can look at the initial output which has all sorts of convergence diagnostics:\n\nwere_fit\n\nInference for Stan model: wereweight.\n4 chains, each with iter=3000; warmup=500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n         mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\nmu     149.23    0.03 2.19  144.90  147.77  149.26  150.69  153.58  6334    1\nsigma   15.59    0.02 1.68   12.66   14.43   15.45   16.59   19.31  6256    1\nlp__  -161.42    0.02 1.03 -164.18 -161.83 -161.11 -160.68 -160.41  3819    1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct 30 23:55:00 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nWe see the Rhat values are high and the n_eff (ESS) is high\nWe also can take a look at trace plots\n\nbayesplot::mcmc_trace(were_fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\nYeti vs Bigfoot - Comparing Two Groups\nLet’s say we had a debate about who was bigger - Yeti or Bigfoot.\nAfter googling and reading some strange websites, I’ve found that there’s quite a range. Bigfoot is pretty consensus at around 220-300kg. Yetis are a bit different. There’s some reports of them being well over 400kg, but other reports that they’re much smaller and, in fact, cuddly.\nWell, I can simulate some data to mimick this range. Then we can analyze it!\n\nData SimulationModelStan CodeAnalysis\n\n\n\nset.seed(1031)\n\n# yetis are log-normal distributed\n\nyeti_lmean = log(130)\nyeti_lsd = log(3)\nyeti_ss = 25\nyeti_obs = rlnorm(yeti_ss, yeti_lmean, yeti_lsd)\n\n\nbigf_mean = 260\nbigf_sd = 20\nbigf_ss = 50\nbigf_obs = rnorm(bigf_ss, bigf_mean, bigf_sd)\n\nI’ve simulated the data to have a lognomal distribution for Yetis, so most will be small but some may be really big. Also because there’s less yetis than bigfeet (common knowledge), we only observe 25 yetis. Bigfoot is more normal and commonly observed.\n\n\nBefore I define the model, I want to make a quick frequentist comparison to illustrate the power in Bayesian analysis.\nIf we want to compare two groups, you might immediately think: t-test!\n\nt.test(bigf_obs, yeti_obs)\n\n\n    Welch Two Sample t-test\n\ndata:  bigf_obs and yeti_obs\nt = 0.076769, df = 24.187, p-value = 0.9394\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -87.90220  94.69695\nsample estimates:\nmean of x mean of y \n 252.9210  249.5237 \n\n\nHmm - a few things to consider here. First - the p-value is very high, suggesting we have no reason to believe from this sample data our two groups have a different value. Well, technically, I did simulate their means to be close, but the have such different distributions, it’s not a good comparison. Second, even if means were appropriate, we shouldn’t be doing a t-test anyways because one of the core assumptions of a t-test is equal variances in our groups:\n\nvar.test(bigf_obs, yeti_obs)\n\n\n    F test to compare two variances\n\ndata:  bigf_obs and yeti_obs\nF = 0.0077948, num df = 49, denom df = 24, p-value &lt; 2.2e-16\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.003694251 0.015102167\nsample estimates:\nratio of variances \n        0.00779483 \n\n\nClearly not equal variances!\nWhat we really should be using to compare our yetis and our bigfeet is the median weights of their population. That could be done using some non-parametric test, but these usually require equal sample sizes between groups - so that is not an option. Technically, you can bootstrap your samples for a median, but that’s outside the standard frequentist approach.\nThis is where we are starting to get a little fun with what we can do with a bayesian model. Since we are interested in comparing certain features of the posterior distribution, we can actually use our posterior.\nFirst we can model the data into our bayesian framework:\n$$\n\\[\\begin{align}\ny_i &\\sim \\mathcal{logN}(\\mu_y, \\sigma_y)\\\\\nb_i &\\sim \\mathcal{N}(\\mu_b, \\sigma_b)\\\\\n\\\\\nlog(\\mu_y) &\\sim \\mathcal{N}(200, 7.38) \\\\\nlog(\\sigma_y) &\\sim \\mathcal{N}(1, 7.38) \\\\\n\n\\mu_b &\\sim \\mathcal{N}(200, 10) \\\\\n\\sigma_b &\\sim \\mathcal{N}(0, 20)\n\\end{align}\\]\n$$\nSo ulimately, I’m interested in comparing the means and we can use the posterior distributions to accomplish this if \\(E(f([\\cdot]))\\) is used to define the expected value of the median for each distribution.\n\\[\nE(f([\\mu_y | y_i])) - E(f([\\mu_g | g_i]))\n\\]\n\n\n\ndata {\n  int&lt;lower=0&gt; N_yeti;\n  int&lt;lower=0&gt; N_bigf;\n  array[N_yeti] real&lt;lower=0&gt; yeti_obs;\n  array[N_bigf] real bigf_obs;\n}\n\nparameters {\n  // Parameters for the Yeti (log-normal distribution)\n  real log_mean;\n  real&lt;lower=0&gt; log_sd;\n  \n  // Parameters for the Bigfoot (normal distribution)\n  real bigf_mean;\n  real&lt;lower=0&gt; bigf_sd;\n}\n\ntransformed parameters {\n  real yeti_median = exp(log_mean);\n}\n\nmodel {\n  // Priors (vague priors, adjust if you have prior knowledge)\n  log_mean ~ normal(5.3, 2);        // Prior on log-mean of Yetis\n  log_sd ~ normal(0, 2);            // Prior on log-SD of Yetis\n  bigf_mean ~ normal(200, 10);      // Prior on Bigfoot mean\n  bigf_sd ~ normal(0, 20);          // Prior on Bigfoot SD\n  \n  // Likelihoods\n  yeti_obs ~ lognormal(log_mean, log_sd);\n  bigf_obs ~ normal(bigf_mean, bigf_sd);\n}\n\ngenerated quantities {\n  // Difference in means on the original scale\n  real median_diff = yeti_median - bigf_mean;\n}\n\nNote in the stan code I compare median to mean but the mean and median are equal in a normal distribution.\n\n\n\n# \\- Prepare data for stan ------------------\n\nmonster_stan_data &lt;- list(\n  N_yeti = yeti_ss,\n  N_bigf = bigf_ss,\n  yeti_obs = yeti_obs,\n  bigf_obs = bigf_obs\n)\n\n\nmonster_fit &lt;- stan(\n  './stan/ex01_two-monsters.stan',\n  'monsters',\n  data = monster_stan_data,\n  iter = 3000,\n  warmup = 500\n)\nmonster_fit\n\n\n\nInference for Stan model: monsters.\n4 chains, each with iter=3000; warmup=500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n               mean se_mean    sd    2.5%     25%     50%     75%   97.5% n_eff\nlog_mean       4.43    0.00  0.28    3.87    4.25    4.43    4.61    4.96  9637\nlog_sd         1.39    0.00  0.21    1.06    1.24    1.37    1.52    1.88  8275\nbigf_mean    252.08    0.04  3.40  245.06  249.88  252.21  254.43  258.37  9069\nbigf_sd       24.03    0.03  2.61   19.64   22.21   23.81   25.61   29.79  8734\nyeti_median   86.78    0.25 24.40   48.13   69.82   83.71  100.11  143.29  9604\nmedian_diff -165.30    0.25 24.57 -204.77 -182.35 -168.30 -151.98 -108.97  9596\nlp__        -238.46    0.02  1.44 -242.14 -239.17 -238.13 -237.40 -236.68  4842\n            Rhat\nlog_mean       1\nlog_sd         1\nbigf_mean      1\nbigf_sd        1\nyeti_median    1\nmedian_diff    1\nlp__           1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct 30 23:55:17 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nWe can see from these quick diagnostics, it ran well. Let’s visualize the two distributions and\n\nmonster_posterior &lt;- rstan::extract(monster_fit)\nggplot() +\n  geom_density(\n    aes(x = monster_posterior$bigf_mean, fill = 'bigfoot'),\n    alpha = 0.5\n  ) + \n  geom_density(\n    aes(x = monster_posterior$yeti_median, fill = 'yeti'),\n    alpha = 0.5\n  ) + \n  geom_point(aes(x = mean(monster_posterior$bigf_mean), color = 'bigfoot', y = 0), size = 4) +\n  geom_point(aes(x = mean(monster_posterior$yeti_median), color = 'yeti', y = 0), size = 4) +\n  geom_segment(\n    aes(\n      y = 0, yend = 0, \n      x = HDInterval::hdi(monster_posterior$bigf_mean)[1], \n      xend = HDInterval::hdi(monster_posterior$bigf_mean)[2],\n      color = 'bigfoot'\n    ),\n    linewidth = 1.5\n  ) +\n  geom_segment(\n    aes(\n      y = 0, yend = 0, \n      x = HDInterval::hdi(monster_posterior$yeti_median)[1], \n      xend = HDInterval::hdi(monster_posterior$yeti_median)[2], \n      color = 'yeti'\n    ),\n    linewidth = 1.5\n  )+\n  scale_fill_manual(values = c('brown', 'lightblue'), breaks = c('bigfoot', 'yeti'))+\n  scale_color_manual(values = c('brown', 'lightblue'), breaks = c('bigfoot', 'yeti'))+\n  labs(x = \"Median Posterior Mass (kg)\", y = \"\", fill = \"\", color = '') +\n  theme_minimal() +\n  theme(legend.position = 'top')\n\n\n\n\nPosterior distribution for the two monsters. Bottom lines show high density 95% credible interval of the posterior median value. Also shown as points are individual observations along the x-axis.\n\n\n\n\n```",
    "crumbs": [
      "Spooky Stuff!",
      "What do we know about monsters?"
    ]
  },
  {
    "objectID": "ex02_hierarchical.html",
    "href": "ex02_hierarchical.html",
    "title": "Hierarchical Horns",
    "section": "",
    "text": "Hierarchical models are really where we start to unlock the power of bayesian analyses. The crux of a hierarchical model is to identify some population can be either made up of multiple sub-populations or maybe part of multiple larger groups. A hierarchical model allows us to identify variation across these different groups\n\nLong horned longhorns:\nI’m diverging from the spooky examples now going to look at longhorns around different ranches. We are interested in the length of steers’ horns - maybe Bevo wants to make sure he has no competition!\nIt’s all the same species, but we can imagine three different ranches which hold longhorn cattle. Let’s say longhorns have an average horn length of 30cm with a standard deviation of 4cm. But we have three ranches. The ranches may all have a different mean value for their cattle, which is some variation of the group mean. I tried really hard to find some data on actual longhorns. However, there is little public information. If anyone wants to go out and measure on the ranch outside Aransas Pass, I’d appreciate it.\nSo we still simulate data:\n\nData simulationModelStan codeAnalysisModel CheckingTake it higher\n\n\n\nset.seed(1031)\n\nrm(list = ls())\nlibrary(ggplot2)\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.6 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(tidyr)\n\n\nAttaching package: 'tidyr'\n\n\nThe following object is masked from 'package:rstan':\n\n    extract\n\nsource('../R/utils.R') #double check this path if you copied\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n#######\n# Different ranches with steers\n#######\n\n# Define parameters\nglobal_mean &lt;- 30\nn_ranches &lt;- 3\ncows_per_ranch &lt;- 15\nsigma_ranch &lt;- 5 # ranch level variation\nsigma_cow &lt;- 2 # individual variation\n\n# simulate ranch specific means with ranch-variation\nranch_means &lt;- rnorm(n_ranches, global_mean, sigma_ranch)\n\n# Simulate data\nranch_data &lt;- data.frame(\n  ranch = rep(c(1:n_ranches), each = cows_per_ranch),\n  cow_id = 1:(n_ranches * cows_per_ranch),\n  horns = c(1:n_ranches) |&gt; \n    lapply(function(r) rnorm(cows_per_ranch, ranch_means[r], sigma_cow)) |&gt; \n    unlist()\n)\n\nggplot(ranch_data) +\n  geom_histogram(\n    aes(\n      x = horns, \n      fill = ranch\n    ),\n    alpha = 0.5\n  ) +\n  facet_wrap(.~ranch) +\n  labs(x = \"Horn Length (cm)\", y = \"Count\")+\n  theme_minimal() +\n  theme(legend.position = 'none')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can see, there some global level of a mean, but it varies by ranch!\n\n\nSo now things start getting a little long with terminology. We’ll call our steer horn lengths \\(y_{ij}\\) for all \\(i\\) observations from each \\(j\\) ranch. Each steer’s horns come from some variaton common across all steers, \\(\\sigma\\). Each ranch has a mean \\(\\mu_j\\), which is drawn from the global mean of all longhorn horns, \\(\\gamma\\). The ranch mean is drawn from a distribution of ranches, with variation \\(\\epsilon\\)\n$$ \\[\\begin{align}\n\ny_i &\\sim \\mathcal{N}(\\mu_j, \\sigma)\\\\\n\n\\mu_j &\\sim \\mathcal{N}(\\gamma, \\tau)\\\\\n\\gamma &\\sim \\mathcal{N}(0,15)\\\\\n\\tau &\\sim \\mathcal{N}(0, 15)\n\n\\end{align}\\] $$\nIn this structure, you may here \\(\\gamma\\) or \\(\\tau\\) referred to as hyperparameters or that they are specified from hyperpriors, which influence the lower observations!\nIt is also common and helpful to draw diagrams (DAGs) to reflect the structure, but I haven’t found a good way to do that on the computer yet.\n\n\n\ndata {\n  int&lt;lower=0&gt; n_steers;\n  int&lt;lower=1&gt; n_ranches;\n  array[n_steers] int&lt;lower=1,upper = n_ranches&gt; ranch_id;\n  vector[n_steers] obs_horns;\n}\n\nparameters {\n  real mu_global;\n  vector[n_ranches] ranch_means;\n  real&lt;lower=0&gt; sigma_cow;\n  real&lt;lower=0&gt; sigma_ranch;\n}\n\nmodel {\n  // Priors\n  mu_global ~ normal(30, 20);\n  ranch_means ~ normal(mu_global, sigma_ranch);\n  sigma_cow ~ normal(0, 15);\n  sigma_ranch ~ normal(0, 15);\n\n  // Likelihood\n  for (n in 1:n_steers) {\n    obs_horns[n] ~ normal(ranch_means[ranch_id[n]], sigma_cow);\n  }\n}\n\n\n\n\n# \\- Stan analysis ------------\nhorn_stan_data &lt;- list(\n  n_steers = nrow(ranch_data),\n  n_ranches = n_ranches,\n  ranch_id = ranch_data$ranch,\n  obs_horns = ranch_data$horns\n)\n\nhorn_fit &lt;- stan(\n  './stan/ex02_ranch-horns.stan',\n  'ranch-horns',\n  data = horn_stan_data,\n  iter = 5000,\n  warmup = 1000\n)\n\n\n\nInference for Stan model: ranch-horns.\n4 chains, each with iter=5000; warmup=1000; thin=1; \npost-warmup draws per chain=4000, total post-warmup draws=16000.\n\n                 mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff\nmu_global       31.00    0.05 3.73  23.13  29.42  31.02  32.57  39.22  5497\nranch_means[1]  33.33    0.01 1.08  31.20  32.62  33.34  34.06  35.43  8258\nranch_means[2]  31.10    0.01 1.02  29.09  30.42  31.10  31.77  33.13 12165\nranch_means[3]  28.58    0.01 1.07  26.47  27.88  28.58  29.28  30.74  6857\nsigma_cow        4.09    0.00 0.46   3.31   3.76   4.05   4.37   5.11 10310\nsigma_ranch      5.42    0.06 4.29   1.15   2.57   4.09   6.77  17.51  5183\nlp__           -87.94    0.03 1.91 -92.54 -89.00 -87.62 -86.51 -85.21  5088\n               Rhat\nmu_global         1\nranch_means[1]    1\nranch_means[2]    1\nranch_means[3]    1\nsigma_cow         1\nsigma_ranch       1\nlp__              1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct 30 11:36:16 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nWe have a lot to parse apart here, but let’s look at a visual:\n\nhorn_post = rstan::extract(horn_fit)\n\nggplot() +\n  geom_density(\n    aes(x = horn_post$mu_global),\n    fill = 'grey', alpha = 0.5\n  )+\n  geom_density(\n    data = horn_post$ranch_means |&gt; \n      as.data.frame() |&gt; \n      pivot_longer(\n        cols = everything(),\n        names_to = 'Ranch', values_to = 'horn_length'\n      ) |&gt; \n      mutate(Ranch = gsub('V', '', .data$Ranch)),\n    aes(x = horn_length, fill = Ranch),\n    alpha = 0.5\n  ) +\n  geom_vline(\n    aes(xintercept = ranch_means, color = as.character(c(1:3))),\n    linewidth = 2\n  ) +\n  #I'm truncating te y xis for clarity\n  scale_x_continuous(limits = c(20, 40)) +\n  labs(x = 'Horn Length (cm)', y = \"\") +\n  theme_minimal()\n\nWarning: Removed 472 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nWe can see in this figure our individual ranch mean posteriors, with the global posterior shown behind in grey. True values are vertical lines.\nOne thing also to consider here is that we simulated the data to all have roughly similar observations. However, a nice feature of a hierarchical set up is the idea of shrinkage - if there is a global level mean, but few observations for a group/individual level mean, those values can effectively borrow from the better observed groups and shink towards the global mean. Conversely it would be difficult to estimate mean with small sample sizes in a frequentist approach.\n\n\nAlso we can do a quick trace plot to see how it ran:\n\nhorn_fit |&gt; \n  mcmc_trace(\n    pars = c(\n      'mu_global', paste0(\"ranch_means[\", 1:3, ']'),\n      'sigma_cow', 'sigma_ranch'\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nIn this example I just set a group-level variation where individual ranches have some variation of a global mean. But you can easily imagine a case where there are different types of cattle. For example let’s say we have a whole population of longhorns with cows, bulls, and steers. They likely all have some effect on the mean horn length depending on their type.\nWe can update our model to include this.\n\\[\n\\begin{align}\ny_{ij} &\\sim \\mathcal{N}(\\mu_{j}, \\sigma)\\\\\n\\\\\n\\mu_{j} &\\sim \\mathcal{N}(\\phi, \\epsilon)\\\\\n\\phi &= \\beta_0 + \\Sigma_{g=1}^3{\\beta_gg_i} \\\\\n\\\\\n\\sigma &\\sim [\\sigma]\\\\\n\\epsilon &\\sim [\\epsilon]\\\\\n\\beta_0 &\\sim [\\beta_0]\\\\\n\\beta_{g_c} &\\sim [\\beta_{g_c}]\\\\\n\\beta_{g_b} &\\sim [\\beta_{g_b}]\\\\\n\\beta_{g_s} &\\sim [\\beta_{g_s}]\\\\\n\\end{align}\n\\]\nIn this case we have a global mean for all longhorns, but it falls up to be the intercept of our model for \\(\\phi\\), which is then impacted by an effect of longhorn-type \\(\\beta_g\\) where \\(g\\) is either cow, bull, or steer. Each effect of \\(\\beta_g\\) has it’s own prior distribution. I added ranch-level variation as \\(\\epsilon\\). Although it could have been more explicitly incorporated. This is a now nested linear model in this system - we’ll look at that next.\nAlso, I said there was individual variation \\(\\sigma\\) which is equal across all ranches and groups, but maybe this isn’t the case, you could then include some hierarchical structure for individual variation!\nYou can see how this quickly can spiral into a larger structure!",
    "crumbs": [
      "Spooky Stuff!",
      "Hierarchical Horns"
    ]
  },
  {
    "objectID": "ex03_linear-models.html",
    "href": "ex03_linear-models.html",
    "title": "Chased by zombies",
    "section": "",
    "text": "Zombie Speed - Linear Regression\nNow let’s image a zombie is chasing you. It’d be good to know how fast a zombie can run. Well, they are definitely motived by hunger. So we can predict that they will run faster if they haven’t eaten for a while. Let’s simulate the data and take a look:\n\n\nSingle Case\n\nData SimulationModelStan CodeAnalysis\n\n\n\nset.seed(1031)\nlibrary(ggplot2) |&gt; suppressMessages()\nlibrary(bayesplot) |&gt; suppressMessages()\nlibrary(rstan) |&gt; suppressMessages()\nlibrary(ggpubr) |&gt; suppressMessages()\n\nrm(list = ls())\nsource('../R/utils.R') |&gt; suppressMessages()\n\n\n\n####\n# Single Case Zombie Speed #####\n####\n\n\nbeta_0 = 2.0\nbeta_1 = 0.5\nsigma = 1\n\nn_zombies = 100\nhours_since = rnorm(n_zombies, mean = 10, sd= 2) # simulate number of hours\nspeed = hours_since |&gt; sapply(function(x) rnorm(1,beta_0 + beta_1 * x,sigma))\n\n\n# simulated data\nggplot() +\n  geom_point(aes(x = hours_since, y = speed)) +\n  stat_smooth(aes(x = hours_since, y = speed), method = 'lm', color = '#5BD08D') +\n  labs(x= 'Hours since eating', y = 'Speed [mph]')+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn this we can use ggplot to show the line of best fit as calculated by OLS, like a normal regression fit.\n\n\nNow we are interested in estimating three parameters related to our model. We have the intercept \\(\\beta_0\\), which is the speed immediately after eating, then we have the \\(\\beta_1\\) slope and variation between individual zombies, \\(\\sigma\\).\nHere I set weakly informative priors:\n\\[\n\\begin{align}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\beta_0 + \\beta_1 * x_i\\\\\n\\beta_0 &\\sim \\mathcal{N}(8,4)\\\\\n\\beta_1 &\\sim \\mathcal{N}(0,10)\\\\\n\\sigma &\\sim \\mathcal{N}(0,5)\n\\end{align}\n\\]\n\n\n\ndata {\n  int&lt;lower=0&gt; N;          // Number of observations\n  vector[N] x;             // Predictor variable\n  vector[N] y;             // Response variable\n}\n\nparameters {\n  real beta_0;             // Intercept\n  real beta_1;             // Coefficient for linear term\n  real&lt;lower=0&gt; sigma;     // Standard deviation of residuals\n}\n\nmodel {\n  vector[N] mu;\n\n  // Priors (weakly informative)\n  beta_0 ~ normal(8, 4);\n  beta_1 ~ normal(0, 10);\n  sigma ~ normal(0, 5);\n\n  // Likelihood\n  y ~ normal(beta_0 + beta_1 * x, sigma);\n}\n\n\n\n\nzombie_data &lt;- list(\n  N = n_zombies,\n  y = speed,\n  x = hours_since\n)\nzombie_single &lt;- stan(\n  './stan/ex03_single-zombie-linear.stan',\n  'zombie-fit',\n  data = zombie_data,\n  iter = 5000, warmup = 1000, cores = 14\n)\n\nzombie_single\n\n\n\nInference for Stan model: zombie-fit.\n4 chains, each with iter=5000; warmup=1000; thin=1; \npost-warmup draws per chain=4000, total post-warmup draws=16000.\n\n         mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nbeta_0   2.33    0.01 0.54   1.28   1.97   2.33   2.70   3.41  4738    1\nbeta_1   0.47    0.00 0.05   0.36   0.43   0.47   0.51   0.58  4744    1\nsigma    1.07    0.00 0.08   0.93   1.02   1.06   1.12   1.23  6386    1\nlp__   -57.03    0.02 1.20 -60.10 -57.58 -56.73 -56.15 -55.64  5182    1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct 30 18:02:12 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nAgain, we can look at the output above to see some quick information about the overall model fit. Let’s plot the results:\nFirst, we can look at how our posterior beliefs changed from the prior:\n\n# \\- Visualizng the results -------------\nzombie_single_post &lt;- zombie_single |&gt; \n  rstan::extract(pars = c('beta_0', 'beta_1', 'sigma'))\n\nb0_plot = posterior_param_plot(\n  zombie_single_post$beta_0,\n  dnorm, seq(1,20, length.out = 1000), beta_0,\n  mean = 8, sd = 4\n) + labs(subtitle = 'beta_0')\nb1_plot = posterior_param_plot(\n  zombie_single_post$beta_1,\n  dnorm, seq(-1, 1, length.out = 1000), beta_1,\n  mean = 0, sd = 3\n)+ labs(subtitle = 'beta_1')\nsigma_plot = posterior_param_plot(\n  zombie_single_post$sigma,\n  dnorm, seq(-3,3,length.out = 1000), sigma,\n  mean = 0, sd = 5\n)+ labs(subtitle = 'sigma')\nggarrange(b0_plot, b1_plot, sigma_plot)\n\n\n\n\n\n\n\n\nNow let’s see how the posterior regression line works:\n\npred_range = make_prediction_data(data.frame(hours_since))\npred_zombie_speed = matrix(NA, length(zombie_single_post$beta_0), ncol = length(pred_range$hours_since))\nfor(i in 1:length(pred_range$hours_since)) {\n  pred_zombie_speed[, i] &lt;- zombie_single_post$beta_0 +\n    zombie_single_post$beta_1 * pred_range$hours_since[i]\n}\npred_rel &lt;- data.frame(\n  mean = apply(pred_zombie_speed, 2, mean),\n  low = apply(pred_zombie_speed, 2, function(x) HDInterval::hdi(x)[1]),\n  high = apply(pred_zombie_speed, 2, function(x) HDInterval::hdi(x)[2])\n)\n\nggplot() +\n  geom_point(aes(x = hours_since, y = speed)) + \n  geom_line(\n    data = data.frame(\n      hours = rep(pred_range$hours_since,100),\n      y = pred_zombie_speed[sample(1:nrow(pred_zombie_speed),100),] |&gt; \n        t() |&gt; \n        as.vector(),\n      line = as.character(rep(1:100, each = length(pred_range$hours_since)))\n    ),\n    aes(x = hours, y = y, group = line),\n    color = 'grey', alpha = 0.25\n  )+\n  geom_line(aes(x = pred_range$hours_since, y = pred_rel$mean)) +\n  geom_ribbon(\n    aes(\n      x = pred_range$hours_since,\n      ymin = pred_rel$low,\n      ymax = pred_rel$high\n    ),\n    fill = '#5BD08D',\n    alpha = 0.5\n  )+\n\n  labs(x= 'Hours since eating', y = 'Speed [mph]')+\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo visualize how our approach is really working, I plotted both the mean and HDI posterior estimates for the regression fit. However, I also randomly sampled 100 (of the 16000 (4 chains x (5000-1000 iterations))) posterior estimates for this relationship. We can see how they vary around the mean, but each has its own estimate! These are shown by the gery lines wile the mean and hdi are black and green respectively.\nAgain if we did this in a frequentist case, it’s really not that different:\n\nlm(speed ~ hours_since) |&gt; summary()\n\n\nCall:\nlm(formula = speed ~ hours_since)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1964 -0.5365 -0.0128  0.7252  2.4063 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.23477    0.54096   4.131 7.61e-05 ***\nhours_since  0.47901    0.05461   8.771 5.60e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.057 on 98 degrees of freedom\nMultiple R-squared:  0.4398,    Adjusted R-squared:  0.4341 \nF-statistic: 76.93 on 1 and 98 DF,  p-value: 5.595e-14\n\n\n\n\n\n\n\nHierarchical Case\nOk well that was stil a pretty straightforward case but it isn’t that realistic. There are different types of zombies, everyone knows this. Zombies can be in parody (funny) movies, scary movies, or video games. And how fast they are will vary across all these different levels. We can use a nested model to describe these different types of zombies.\n\nData SimulationModelStanAnalysis\n\n\nFor this simulation, I’m keepign the same predictors as before with the same slope, I’ve just now added a type_effect corresponding to which zombie it is.\n\ntype_effect = c(0,2,10)\nsigma = 1\n\nn_zombies = 100\nhours_since = rnorm(n_zombies, mean = 10, sd= 2) # simulate number of hours\ntype = sample(c('funny', 'scary', 'video_game'),n_zombies, replace = T) |&gt; as.factor()\nspeed = beta_1 * hours_since + type_effect[type] + rnorm(n_zombies, 0, sigma)\n\n\n# simulated data\nggplot() +\n  geom_point(aes(x = hours_since, y = speed, color = type)) +\n  stat_smooth(aes(x = hours_since, y = speed, color = type), method = 'lm') +\n  labs(x= 'Hours since eating', y = 'Speed [mph]')+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nNow our model is nested to reflect that there will be different intecepts for each type of zombie. If we are trying to predict the speed of a single zombie, we can describe that as \\(y_{it}\\) for the \\(i^{th}\\) zombie of type \\(t\\)\nHonestly, the way I constructed this model, I didn’t constrain the effect to 0, with some standard intercept as mean, instead I have the group intercepts drawn from a common mean intercept (\\(\\phi\\)) which is nested above - all to say I may have written this slightly wrong with respect to indexing.\nThe slope of the effect of speed is \\(\\beta_h\\) and the intercept for each group is \\(\\beta_t\\) $$\n\\[\\begin{align}\n\ny_{it} &\\sim \\mathcal{N}(\\mu_t, \\sigma)\\\\\n\\\\\n\\mu_t &= \\beta_h * h_i + \\Sigma_t \\beta_t * t_i \\\\\n\\beta_t &\\sim \\mathcal{N}(\\phi, \\tau)\\\\\n\\\\\n\\sigma &\\sim \\mathcal{N}(0,5)\\\\\n\\beta_h &\\sim \\mathcal{N}(0,10)\\\\\n\\phi &\\sim \\mathcal{N}(0,10)\\\\\n\\tau &\\sim \\mathcal{N}(0,5)\\\\\n\n\\end{align}\\]\n$$\n\n\n\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; n_groups;\n  vector[N] x;\n  vector[N] y;\n  array[N] int&lt;lower=1,upper=n_groups&gt; type;\n}\n\nparameters {\n  real beta_1;\n  real group_mean;\n  vector[3] group_intercept;\n  real&lt;lower=0&gt; sigma;\n  real&lt;lower=0&gt; tau;         // Standard deviation of group effects\n}\n\nmodel {\n  vector[N] mu;\n\n  // Prior distributions\n  beta_1 ~ normal(0, 10);\n  group_mean ~ normal(0,10);\n  sigma ~ normal(0, 5);\n  tau ~ normal(0, 5);\n\n  for(i in 1:n_groups) {\n    group_intercept ~ normal(group_mean, tau);\n  }\n\n  for (i in 1:N) {\n    mu[i] = group_intercept[type[i]] + beta_1 * x[i];\n  }\n  y ~ normal(mu, sigma);\n}\n\n\n\n\n# \\- Run the stan -----------------------\n\nhier_zdata = list(\n  N = n_zombies,\n  n_groups = length(type_effect),\n  x = hours_since,\n  y = speed,\n  type = as.numeric(type)\n)\n\nhz_fit &lt;- stan(\n  './stan/ex03_hierarchical-zombie.stan',\n  'hz',\n  data = hier_zdata,\n  iter = 5000, warmup = 1000, cores = 14\n)\nhz_fit\n\n\n\nInference for Stan model: hz.\n4 chains, each with iter=5000; warmup=1000; thin=1; \npost-warmup draws per chain=4000, total post-warmup draws=16000.\n\n                     mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff\nbeta_1               0.51    0.00 0.05   0.42   0.48   0.51   0.54   0.61  5699\ngroup_mean           3.76    0.02 1.69   0.33   2.67   3.77   4.85   7.06  9379\ngroup_intercept[1]   0.00    0.01 0.48  -0.95  -0.31   0.01   0.32   0.95  5984\ngroup_intercept[2]   1.77    0.01 0.49   0.80   1.45   1.77   2.10   2.73  5862\ngroup_intercept[3]   9.87    0.01 0.47   8.92   9.56   9.87  10.19  10.80  5957\nsigma                0.93    0.00 0.07   0.80   0.88   0.92   0.97   1.07 11170\ntau                  4.94    0.01 1.25   3.12   4.05   4.73   5.62   7.95  8476\nlp__               -59.37    0.02 1.91 -63.93 -60.44 -59.06 -57.96 -56.63  6414\n                   Rhat\nbeta_1                1\ngroup_mean            1\ngroup_intercept[1]    1\ngroup_intercept[2]    1\ngroup_intercept[3]    1\nsigma                 1\ntau                   1\nlp__                  1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct 30 18:02:58 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nLooks pretty good but we can go ahead and visualize this for more clarity:\n\n# \\- plot this output --------------\nhz_post = extract(hz_fit, pars = c('beta_1', 'group_intercept'))\nhz_pred_speed = list(\n  matrix(NA, length(hz_post$beta_1), ncol = length(pred_range$hours_since)),\n  matrix(NA, length(hz_post$beta_1), ncol = length(pred_range$hours_since)),\n  matrix(NA, length(hz_post$beta_1), ncol = length(pred_range$hours_since))\n)\n\nfor(i in 1:length(hz_pred_speed)) {\n  for(j in 1:length(pred_range$hours_since)) {\n    hz_pred_speed[[i]][, j] &lt;- hz_post$group_intercept[,i] +\n      hz_post$beta_1 * pred_range$hours_since[j]\n  }\n}\n\npred_hz &lt;- hz_pred_speed |&gt; \n  lapply(\n    function(x) \n    data.frame(\n      mean = apply(x, 2, mean),\n      low = apply(x, 2, function(k) HDInterval::hdi(k)[1]),\n      high = apply(x, 2, function(k) HDInterval::hdi(k)[2])\n    )\n  ) \nfor(i in 1:3) {\n  pred_hz[[i]]$type = c('funny', 'scary', 'video_game')[i]\n}\n\npred_hz &lt;- pred_hz |&gt; \n  do.call(what = rbind,)\n\nggplot() +\n  geom_point(aes(x = hours_since, y = speed)) + \n  geom_line(\n    aes(x = rep(pred_range$hours_since,3), y = pred_hz$mean, color = pred_hz$type)\n  ) +\n  geom_ribbon(\n    aes(\n      x = rep(pred_range$hours_since,3), \n      ymin = pred_hz$low,\n      ymax = pred_hz$high, \n      fill = pred_hz$type),\n    alpha = 0.5\n  )+\n  labs(x= 'Hours since eating', y = 'Speed [mph]')+\n  theme_minimal()\n\n\n\n\n\n\n\n\nFun. It is worth noting that this is essentially a mixed effects model which can be easily fit using a frequentist case with REML instead of OLS - in lme4. However, the bayesian case give us a little more control over our priors and we could easily do more like adding some individual level variation away from the group-level mean. I kept things simple here but it is easy to add layers!",
    "crumbs": [
      "Spooky Stuff!",
      "Chased by zombies"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spooky Stats: A brief introduction to Bayesian modelling in R",
    "section": "",
    "text": "What this is:\nI put together this page as a set of examples of why Bayesian inference can be useful or exciting. I’m assuming users have some familiarity with statistics or probability, likely from at least one undergrad level course. I’m also assuming you have some familiarity with working in R as that is what I use for all these examples.\nThese examples should offer a starting point or motivation for constructing your own analyses. I constucted these examples from an ecology/marine science mindset. However, because I wrote these examples to share on Halloween, they are extra spooky. So, these examples are hopefully thought-provoking for both ecologists, marine scienstists and paranormal researchers or cryptozoologists.\n\nReplicating these examples:\nThis website is hosted from a github repo. You can copy and paste code from each article to construct your own analyses. However, you can also access all the scripts - not in a notebook format - directly from the github repo. If you clone the repo, you can run the examples from each script and it will nicely integrate with all the data I pull in some of the more complicated examples.\n\n\n\nWhat this is not:\nThis is not a comprehensive introcution, or even close to it. I constructed this to be a short introduction as a motivation or starting point for your own investigation. While the code in these examples may be a useful reference, it does not contain all the information you will need to conduct, and importantly - check, your own analyses. I largely don’t cover the concepts behind MCMC, probability distributions, or the underlying mechanics for Bayesian inference. You definitely should explore more as there are resources better than I could write which can guide you.\n\nAlso, while in the provided examples, I do share some methods for reporting results of Bayesian analyses, it is far from a comprehensive guide of what should be shared.\nTake a look at the additional resources\nAlso, this is not an argument for the existence of ghosts, werewolves, or zombies - just attempts to update our beliefs on their probabilities.\n\n\nSlides:\nHere are the slides from my first presentation of these examples.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "ex05_process-models.html",
    "href": "ex05_process-models.html",
    "title": "Processing it all",
    "section": "",
    "text": "Coming Soon\n\nset.seed(1031)\nrm(list = ls())\n\n# Load necessary libraries\nlibrary(deSolve)\nlibrary(ggplot2)\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.6 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\n# Define the Lotka-Volterra model\nghosts_dyanos &lt;- function(t, state, parameters) {\n  with(as.list(c(state, parameters)), {\n\n    dGhosts &lt;- g_arrival * ghosts - g_removal * ghosts * busters\n    dBusters &lt;- b_arrival * ghosts * busters - b_exit * busters\n    \n    list(c(dGhosts, dBusters))\n  })\n}\n\n# Parameters\nparameters &lt;- c(g_arrival = 0.1,\n                g_removal = 0.002,\n                b_arrival = 0.002,\n                b_exit = 0.1)\n\n# Initial state\nstate &lt;- c(ghosts = 40, busters = 9)\n\n# Time sequence\ntimes &lt;- seq(0, 100, by = 1)\n\n# Run the simulation using ode_rk45\nout &lt;- ode(y = state, times = times, func = ghosts_dyanos, parms = parameters, method = \"rk4\")\n\n# Convert output to a data frame\nout_df &lt;- as.data.frame(out)\n\n\n\n# \\- Simulate observations ------------------------\nb_times &lt;- seq(1,max(times),2)\nbuster_sightings &lt;- out_df$busters[b_times] |&gt; \n  sapply(function(x) rnorm(1, x, 0.01)) |&gt; # imperfect sightings\n  sapply(function(x) max(0,x))\n\ng_times &lt;- sample(c(1:max(times)), 5) |&gt; \n  sort()\n\nghost_sightings &lt;- out_df$ghosts[g_times] |&gt; \n  sapply(function(x) rnorm(1, x, 2)) |&gt; \n  sapply(function(x) max(0,x))\n\n# Plot the results\nggplot() +\n  geom_line(\n    data = out_df,\n    aes(y = ghosts, color = \"Ghosts\", x= time)\n  ) +\n  geom_line(\n    data = out_df,\n    aes(y = busters, color = \"Busters\", x= time)\n  ) +\n  geom_point(\n    aes(\n      x = b_times,\n      y = buster_sightings,\n      color = \"Busters\"\n    )\n  )+\n  geom_point(\n    aes(\n      x = g_times,\n      y = ghost_sightings,\n      color = \"Ghosts\"\n    )\n  )+\n  labs(x = \"Time\", y = \"Group Density\") +\n  scale_color_manual(\"\", \n                     breaks = c(\"Ghosts\", \"Busters\"),\n                     values = c(\"blue\", \"red\")) +\n  theme_minimal()"
  },
  {
    "objectID": "01_quick_bayes_introduction.html",
    "href": "01_quick_bayes_introduction.html",
    "title": "Probably Useful Information",
    "section": "",
    "text": "Probability\nMany ecologists learn some basic statistics in undergrad, with maybe some advanced training in grad school. However, we mostly learn through other papers or patchwork approaches while trying to analyze our own data. As a consequence, there isn’t always a formal introduction to basic concepts of probability. However it is important for us consider some of the fundamental ideas underneath this concept of “probability” which we are using when we conduct our analyses.\nSimilar to the words “hypothesis” and “theory”, words like “probability”, “chance”, “odds” are common to use in english but the connotation of these words is not often considered. When talking about statistics, the term “probability” is quite a loaded word and may be interpretted in a number of ways. Bulmer (1965)’s first chapter offers a nice discussion on the theoretical implications of two concepts of probability, which he defines as “statistical probability” and “inductive probability”. Similarly, Kruschke (p73-77) discusses these two terms as “outside the head” and “inside the head” respectively (possibly a more approachable read given modern writing). Hopefully not to complicate it further, I’m going to use the terms “frequency” and “credibility” to breifly discuss these two different probability concepts.\n\nFrequency Definition\n\nThis is the more formal definition of probability which is likely introduced in a class on probability. It is the idea that probability is a measure of a long-run relative frequency of some specified event. The classic example is that the probability of getting heads on a fair coin flip is 0.5, which can be demonstrated by flipping a coin some large number of times and calculating the proportion of head. Authors will often refer to this as an empirical approach to probability as it can be demonstrated (hence Kruschke’s “outside the head” term as it can occur in the real world).\nWe can simulate such a probability to see as our sample size increases, the measured frequence of heads converges to the fixed true probability value:\n\nset.seed(1031)\nlibrary(ggplot2)\n\ntrue_prob = 0.5\nnum_trials = seq(1,1000,1)\ncoin_result = sapply(num_trials, function(x) rbinom(1, x, true_prob)) / num_trials\n\nggplot() +\n   geom_line(aes(x = num_trials, y = coin_result)) +\n   labs(x = 'Number of Coin Flips', y = 'Frequency of Heads') +\n   theme_minimal()\n\n\n\n\n\n\n\n\nClearly, increasing the number of trials allows us to converge towards the true probability.\n\nCredibility Definition\n\nAlternative to the frequency interpretation of probability, we have a more colloquial use of probability. This is like “what’s the probability Texas makes it to the SEC championship game” or “What’s the probability the ghosts of all the fish we’ve caught will haunt us forever?”\nThis use of the term probability is more referring to the credibility of some event or the strength of belief in a statement. This terminology, while more intuitive from a thinking standpoint, is less straightforward to define in a mathematical concept. Thus, many approaches of orthodox statistics try to hammer us away from using this terminology of probability.\nTo quote the end of Bulmer’s 1965 introduction to Principles of statistics:\n\nSuch a psychological scale varying from one person to another is of little use in scientific discussion in which the scientist mush persuade others to believe what he himself believes.\nIt has been reluctantly concluded by most statisticians that inductive probability cannot in general be measured and, therefore, cannot be used in the mathematical theory of statistics… …there seems no reason why rational degrees of belief should be measurable any more than, say, degrees of beauty… it does not seem possible to construct a numerical scale of such (inductive) probabilities.\n\nThis is quite the argument against the credibility definition of probability. However, the basic idea of Bayesian analysis does exactly what Bulmer describes as impossible: assign mathematical measurements to characterize levels of belief (or credibility) to some event or value. Both Gelman (2013) and Kruschke (2015) offer criticisms with the frequency definition - and how it is not as objective as the frequentists claim. I don’t go into those arguments here but to quote Gelman Chapter 1:\n\nIn Bayesian statistics, probability is used as the fundamental measure or yardstick of uncertainty. Within this paradigm, it is equally legitimate to discuss the probability of ‘rain tomorrow’ or of a Brazilian victory in the soccer Wold Cups as it is to discuss the probability that a coin toss will land heads…\n… Bayesian methods enable statements to be made about the partial knowledge available (based on data) concerning some situation or ‘state of nature’ (unobservable or as yet unobserved) in a systematic way, using probability as the yardstick. The guiding principle is that the state of knowledge about anything unknown is described by a probability distribution.\n\n\n\nNotation & Definitions:\n\nProbability distributions & random variables\nWhen describing a model, we will use notation to define which distributions we are using to model our parameters and data-generating processes. So it is important to be familiar with these things. A probability distribution, to quote Kruschke, is “simply a list of all possible outcomes and their corresponding probabilities”. Effectively, it is a function used to describe the distribution of probabilities around some event. There are several probability density (or mass for discrete data) functions which can be used to describe distributions. For example, the binomial distribution would be used to define the coin-toss example. Alternatively, a normal distribution may be used to describe the weight of werewolves from some population.\nA variable is said to be a random variable if it is value is generated by a stochastic process (sampled by a probability distribution). For example, if werewolf weight, \\(W\\), can be described by a normal distribution with mean, \\(\\mu\\), and variance, \\(\\sigma^2\\), we can write \\(W \\sim \\mathcal{N}(\\mu, \\sigma^2)\\). Probabilitistic statements can then be written as \\(Pr(W = w) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{1}{2}(\\frac{w-\\mu}{\\sigma})^2)\\) to say “the probability any given werewolf weighs \\(w\\) is equal to the probability density function (PDF) for a normal distribution evaluated at \\(w\\)”.\nIn bayesian analyses we end up writing a lot of distributions and notation. Since it is not always easy to write out the distributions specified, or because we may not be referring to a defined distribution, we need a short hand for specifying the PDF. Some authors (Gelman, Kruschke) use the notation \\(p(\\cdot)\\). However, this becomes cumbersome when writing out more detailed models or formulations. In most cases, ecologists may not encounter this as a huge issue (I’ve seen papers with \\(\\theta \\sim Dist(\\theta)\\) which is even messier). However, it is fairly common in Bayesian literature to use bracket notation, \\([\\cdot]\\) for a PDF.\nSo, in our example: \\[\n\\begin{split}\nW \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\equiv\nW \\sim p(W)\n\\equiv\nW \\sim [W]\n\\end{split}\n\\]\nNote in this example, I used a normal distribution but \\([\\cdot]\\) can be any distribution in practice.\n\n\nConditional Probability\nFull probability theory and introduction is way outside the scope of what’s intended here. But it is important to define the notation of conditional probability. We use the notation \\([a | b]\\) to say the probability of \\(a\\) conditional on \\(b\\). In other words if we know \\(b\\), what is the probability of \\(a\\)?\nThe formula of conditional probability can be defined as:\n\\[\n[a|b] = \\frac{[a, b]}{[b]}\n\\]\nThe joint distribution of \\([a,b]\\) is the product probability of both \\(a\\) and \\(b\\) conditioned on \\(a\\):\n\\[\n[a,b] = [a][b|a]\n\\]\nsee Hooten & Hefley chapter 1 for all details\n\n\nBayes Rule:\nNow that we have some basics, we can define the Bayes rule which underlies this whole idea:\nUsing the conditional distribution \\([a|b]\\), we can substitute in the definition of \\([a,b]\\) \\[\n\\begin{align}\n[a|b] &= \\frac{[a, b]}{[b]}\\\\\n\\\\\n&= \\frac{[a][b|a]}{[b]}\n\\end{align}\n\\]\nThis is the fundamental concept behind Bayesian analysis which I’ll discuss below. For a fun bit of history, Bayes rule was named after Reverend Bayes but Bayes did not publish his own work and left the implications for it somewhat unfinished. It was Richard Price who discovered his work and published it in 1763. However, it is worth noting that really it was Laplace who independently developed Bayes’s rule and published it in 1774 with more direct applications. Despite the early definitions of Bayes rule, which as you’ll see allows its application to mathematically formulate inductive probabilities, it was not widely adopted. Most of statistics throughout the 20th century was descended from Fisher’s work on frequentist analyses - which places Bulmer’s comments into much clearer context. As you’ll see, Bayesian analysis, while not exclusively, typically typically rely stochastic simulations which require strong computational resources. So it hasn’t been until the past couple of decades Bayesian analyses are approachable for modern cases. As computation resources increase, the power of bayes only grows.\n\n\n\nApplications to analyses\nNow with a basic grasp of the fundamental pieces behind probability - let’s discuss briefly how this all works its way into statistical inference.\n\nFrequentist statistics\nIf you’ve run any analyses on your data there is a good chance you’ve used what are referred to as frequentist statistics. These are essentially anything which rely on a p-value to interpret results. These analyses all rely on the long-run frequency interpretation of probability\nExplaining all of frequentist statistics is a long winded In short the idea regarding frequentist analysis is that we are interested in some population level parameter which has a true, fixed value. We then collect a sample from this population and using our sample statistics, we can use some inferential statistical analyses (or tests) to try and make inferences about that parameter. In past classes, I used this diagram to describe this process:\n\nWhen statistically testing our samples for population level inference, we are saying, “if I collected this sample many, many repeated times what would my sample look like?”. Thus most tests will report both a confidence interval and p-value. First, let’s talk confidence intervals. Usually a stats course might teach students this means, “I am 95% confident that the true population-level parameter is within this range.” Although, more technically the confidence interval could be thought in a frequentist context of “If the we sampled from the population distribution many, many times 95% of the time, our sample would capture the true parameter value”. I always found this concept very well characterized by an app made by the professor of my undergraduate probability course linked here\nA p-value is an extension of the confidence interval in some ways. What most students will learn, and use, is that a small p-value means that your findings are “statistically significant”. But what does that really mean? Generally, a p-value is a result from some test which in technical terms is the probability of a type-i error, or rejecting the idea that your parameter of interest is different from some other value (usually 0). In more approachable language - a small p-value suggests that your parameter is statistically significantly different than 0 (or some other value). Thinking in frequentist ideas, it is the probability you would draw that sample of data, if the true population parameter is 0. However it is in general sticky to explain and why many students (and scientists) do not correctly discuss their results.\nOften times, I think ecologists are not actually interested in the results of the frequentist statistical analyses which they are running. However, this is often all which is learned and there is the never-ending hunt for a “statistically significant” result which unfortunately drives us in the wrong direction. One thing, which is well explained in Kruschke’s chapter on NHST, is that the p-value really depends on sample size in most cases and can bias our conclusions (See figure 11.1).\nI think that these analyses do have their place but are often misunderstood and misused. These short sentences are far from a full discussion on the advantages and challenges of frequentist analyses but see some of these resources:\n\nKruschke chapter 11\nSullivan & Feinn 2012\nThis vox article\nDushoff et al. 2019 (I like this one a lot)\n\nAgain this was far from a complete introduction to frequentist stats, but here’s an undergraduate-level reference sheet I wrote a while ago: link\n\n\nBayesian analysis:\nSo what we’ve all waited for - Bayesian analysis. The fundamental idea of Bayesian analysis is that everything is treated as a random variable, meaning it is drawn from some distribution. So if we are interested in a population parameter - we don’t assume it has some true fixed value, but rather we are interested in describing the distribution of that parameter. We can use bayes rule to describe parameter distribution using both prior knowledge of that parameter, \\(\\theta\\), and observed data, \\(y\\).\nThis is better described to returning our Bayes rule,\n\\[\n[\\theta | y] = \\frac{[y | \\theta][\\theta]}{[y]}\n\\]\nThis effectively is saying what’s the probability of \\(\\theta\\), or a hypothesis, given \\(y\\), or data I’ve observed. So in words:\n\\[\n[Hypothesis | data] = \\frac{[data | Hypothesis][Hypothesis]}{[data]}\n\\]\nLet’s break down each part of this into words:\n\n\\([\\theta|y]\\) - the probability distribution of a parameter given data. This is called the posterior distribution\n\\([y| \\theta]\\) - the probability of that data given the parameter value. This is often called the likelihood of the data or in practice we will define some model, called a “data model” where we define some process which generates data under the parameters of interest.\n\\([\\theta]\\) - the probability of that parameter. This is referred to as the prior distribution. Here, we can incorporate prior knowledge or assumptions about the parameter\n\\([y]\\) - the probability of the data under the model. Also referred to as evidence or marginal likelihood\n\nThe last piece here is the most complicated as it can be tricky to calculate as it is the probability of \\(y\\) under all possible values of \\(\\theta\\). This means:\n\\[\n[y] = \\int [y|\\theta][\\theta]d\\theta\n\\]\nHowever, by integrating over all possible values of \\(\\theta\\), \\([y]\\) will be calculated as a constant. As such, it only serves to scale the numerator in bayes rule and is not actually necessary to describe the posterior distribution. So we can update our formula to say:\n\\[\n[\\theta | y] \\propto [y | \\theta][\\theta]\n\\]\nThus, our posterior distribution is proportional to the data model (likelihood) and the prior distribution. In practice, this means we can take our prior belief (\\([\\theta]\\)) and update it based on data. To quote Kruschke’s opening lines of Chapter 2: “Bayesian inference is reallocation of credibility across possibilities”. We can specify what we thought about something, then based on new information update our belief. Not only does this allow for the use of the inductive terminology of probability, it much more closely matches our intuitive way to discuss science. What do we think about a hypothesis before a study and how does this update when we finish the study. For a spooky example: Let’s say you firmly believe your house is haunted, then we bust out all the tools to look for ghosts and find absolutely nothing, your updated belief in the haunted house likely declined a bit.\nIn Bayesian data analyses we typically don’t concern ourselves with Null-hypothesis significance testing (p-values). Instead, we often will report credible intervals - a range of values which best captures the bulk of the posterior distribution. These can be 95% quantiles, however more often a high-density interval is used as it can capture non-uniform distributions.\nAll this will hopefully become clear in our examples upcoming:",
    "crumbs": [
      "Basics",
      "Probably Useful Information"
    ]
  },
  {
    "objectID": "02_mcmc_tech.html",
    "href": "02_mcmc_tech.html",
    "title": "Describing the posterior",
    "section": "",
    "text": "Why?\nThe aim of Bayesian inference is to describe \\([\\theta | y]\\), our posterior distribution. Then we can describe things like the posterior mean and credible intervals. However, describing the posterior distribution is a challenge. Think about how we updated bayes rule to this proportionality statement: \\([\\theta | y] \\propto [y | \\theta][\\theta]\\).\nSo to calculate characteristics of the posterior, we’d need to be able to describe it. Since we are removed the demoninator (\\([y]\\)), it becomes difficult. In some limited cases, we can have the posterior match the form of the prior (conjugate) and we can define all cases for the marginal likelihood, allowing us to solve for the posterior analyticalt. But such cases are rare.\nAs a result, Bayesians use stochastic methods to approximate the posterior distribution\n\n\nMonte Carlo Methods\n\n\n\n\n\n\nWarning\n\n\n\nThe overall concepts behind MCMC methods or other stochastic approximation tools are far beyond the scope of what I’ve put together here. It is worth reading a little more detail elsewhere. Probably the most comprehensible, yet still accessible, introduction to MCMC is in Hooten & Hefley 2015. Gelman 2013 Chpater 10 provides a lot of detail and Kruschke Chapter 7 also provides a pretty useful introduction to Gibbs sampling with some general model-checking resources.\nHere, I try to summarize those works and provide a short, but useful start.\n\n\nThe posterior distribution can be approximated by generating a really large sample from it. To show this concretely take an example with a normal distribution. While we know the form of a normal distribution, we can approximate it by generating a large number of random samples from that distribution, this process is called Monte Carlo (MC) sampling.\n\nset.seed(1031)\nsample_size = 100\ndata = rnorm(sample_size)\nhist(data, probability = TRUE, main = \"Histogram with Normal Curve\")\ncurve(dnorm(x, mean = mean(data), sd = sd(data)), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nqqnorm(data)\nqqline(data, col = 'blue')\n\n\n\n\n\n\n\n\nWith only 100, it doesn’t do too good a job approximating the distribution but if you run the examples and increase to 1000, it’ll work a little better. (Even better with more!)\nWell this makes sense with a known distribution, we don’t know our posterior distribution. So what is done for Bayesian computation is generate a posterior using “guesses” for values of \\(\\theta\\). This can be denoted as \\(\\theta^* \\sim [\\theta]\\). For a bunch of guesses, we can generate a whole samples of \\(\\{\\theta^{(1)}, ..., \\theta^{(k)}\\}\\) for \\(k\\) guesses.\nSince we are interested in features of the parameter (like mean or high density intervals), we can define a generic function of the distribution as \\(\\mathcal{f}(\\theta)\\). So expectated values of the poseterior distribution would be \\(E(\\mathcal{f}(\\theta) | y)\\). With this in mind:\n\\[\n\\begin{align}\nE(\\mathcal{f}(\\theta) | y) &= \\int \\mathcal{f}(\\theta) [\\theta | y]d\\theta \\\\\n&\\approx \\sum_{k=1}^{k}{\\mathcal{f}(\\theta^{(k)})}\n\\end{align}\n\\] (equations 10.1 gelman and 3.1 hooten & helfley)\nThe vector of \\(\\theta^{(k)}\\) can be generated by MC sampling. Often this is Markov Chain MC (MCMC), although other methods exist, notably HMC (see below). Without going into detail, this process works by using some algorithm to generate suggestions for new values of \\(\\theta\\), which are added based on some criteria to the vector \\(\\theta^{(k)}\\). These algorithms will propose and accept values based on data and prior specifications of \\([\\theta]\\). This then is used to approximate our posterior distribuiton.\nAll MC algorithms will genereate the vector of \\(\\theta^{(k)}\\) for some user specified length. These vectors are often referred to as “chains”. You can think of the chain as working its way around the distribution, so the more time it has to explore the parameter space, the better. Chain length is an important consideration as it will improve estimates of the posterior to have longer chains, but this will increase computation cost. 2000 is a good minimum but this depends on the overall number of parameters.\nSimilarly, multiple chains are run (typically 3-4) and this allows multiple attempts at the posterior distribution. The chains can be compared for convergence to evaluate how well the model is working.\nSince the chain starts at some (usualy random) initial value, it needs time to work its way into the center of the high-density regions of the posterior distribution. So usually some amount of the beginning fo the chain is discarded, the amount of discarded MC samples is called burn-in\nFinally, sometimes chains are thinned, where samples are removed from the posterior at some rate. This typically is only done to reduce memory constraints. However many intro tutorials may make this a routine item, but it is not necessary and may even be detrimental.\nSo far, I’ve mostly provided examples in the case of a single-parameter, but as you may have guessed from the langauge and will see in this example, posterior distribution may have multiple, hierarchical parameters so it is estimating a joint posterior distribution across many dimensions - clearly not so simple!\n\n\nTechnology\nYou don’t have to know how to build an engine to be able to drive a car. And thankfully, you don’t have to write your own algorithms in order to conduct Bayesian analyses. There are several tools which play nicely with R to conduct your analyses. Here I use stan but there are several others. See the additional resources tab.",
    "crumbs": [
      "Basics",
      "Describing the posterior"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "03_Model-Checking.html",
    "href": "03_Model-Checking.html",
    "title": "Model Checking",
    "section": "",
    "text": "The process of model checking for a Bayesian analysis involves two steps: seeing if the estimation of the posterior was good, then seeing if the model fit is actually useful. The assumptions of a Bayesian analysis are slightly more relaxed compared to traditional frequentist analyses but it requires a bit more thought when constructing and evaluating the model. You should be able to justify your subjective decisions in model building.\n\n\nThe first thing to consider when fitting a Bayesian model is if your MC Chains are well mixed, or converged. This means that you are doing a good job estimating the posterior as the chains have explored, and largely landed, in simimlar regions of high density for values of the posterior. There are several diagnostic tools to check for model convergence, many of which are automatically generated in output from stan or easily accessible using functions in r (check out bayesplot).\nSome things which are worth considering:\n\nRhat: this is a metric evaluating how well the chains converged. It should be near 1 (stan says acceptable around &lt;1.1) but this can be context dependent.\nESS: Effective sample size is a metric which measures how many samples can be considered independent from your chains. While I didn’t go into detail, since MCMC estimation of the posterior relies on sequentially adding proposal values for \\(\\theta\\), those values are inherently autocorrelated so ESS calculates a value for how many can be considered independent while accounting for an autocorrelation factor (Kruschke equation 7.11).\nTraceplots: you can visualize the MCMC chains using things like bayesplot::mcmc_trace(fit) to see if the chains are well mixed. This also is a good oppoturnity to see if there was the propper amount of burn-in.\n\n\n\n\nOnce your confirmed if the chains are well mixed, you can see if the posterior distribution and overall model structure are actually useful for your data. This can be done a number of ways:\n\nPPC: Posterior Predictive Checks involve using the model to simulate what is expected with the estimation of the posterior distribution. Comparing these estimates allows us to see how well the predictions match the observed data. This is fundamentaly just like checking residuals and you can even do all the residual plots which you might be familiar with. This is easily done using the bayesplot package in r with pp_check()\nYou can check residuals using common residual tests or things like calculating RMSE.\nBayes P-value: A bayes p-value checks if your predicted data match the observed data using some discrepancy metric\nCross-validation: This is an approach more common in ML applications but works really well here. Esspecially applicable if you are interseted in using your posterior distributions for prediction or forecasting. These procedures involve selectively leaving out some of your data, then evaluating the model’s ability to predict that data (leave-one-out or k-fold are common approaches).",
    "crumbs": [
      "Basics",
      "Model Checking"
    ]
  },
  {
    "objectID": "03_Model-Checking.html#convergence-diagonsitics",
    "href": "03_Model-Checking.html#convergence-diagonsitics",
    "title": "Model Checking",
    "section": "",
    "text": "The first thing to consider when fitting a Bayesian model is if your MC Chains are well mixed, or converged. This means that you are doing a good job estimating the posterior as the chains have explored, and largely landed, in simimlar regions of high density for values of the posterior. There are several diagnostic tools to check for model convergence, many of which are automatically generated in output from stan or easily accessible using functions in r (check out bayesplot).\nSome things which are worth considering:\n\nRhat: this is a metric evaluating how well the chains converged. It should be near 1 (stan says acceptable around &lt;1.1) but this can be context dependent.\nESS: Effective sample size is a metric which measures how many samples can be considered independent from your chains. While I didn’t go into detail, since MCMC estimation of the posterior relies on sequentially adding proposal values for \\(\\theta\\), those values are inherently autocorrelated so ESS calculates a value for how many can be considered independent while accounting for an autocorrelation factor (Kruschke equation 7.11).\nTraceplots: you can visualize the MCMC chains using things like bayesplot::mcmc_trace(fit) to see if the chains are well mixed. This also is a good oppoturnity to see if there was the propper amount of burn-in.",
    "crumbs": [
      "Basics",
      "Model Checking"
    ]
  },
  {
    "objectID": "03_Model-Checking.html#evaluating-model-fit",
    "href": "03_Model-Checking.html#evaluating-model-fit",
    "title": "Model Checking",
    "section": "",
    "text": "Once your confirmed if the chains are well mixed, you can see if the posterior distribution and overall model structure are actually useful for your data. This can be done a number of ways:\n\nPPC: Posterior Predictive Checks involve using the model to simulate what is expected with the estimation of the posterior distribution. Comparing these estimates allows us to see how well the predictions match the observed data. This is fundamentaly just like checking residuals and you can even do all the residual plots which you might be familiar with. This is easily done using the bayesplot package in r with pp_check()\nYou can check residuals using common residual tests or things like calculating RMSE.\nBayes P-value: A bayes p-value checks if your predicted data match the observed data using some discrepancy metric\nCross-validation: This is an approach more common in ML applications but works really well here. Esspecially applicable if you are interseted in using your posterior distributions for prediction or forecasting. These procedures involve selectively leaving out some of your data, then evaluating the model’s ability to predict that data (leave-one-out or k-fold are common approaches).",
    "crumbs": [
      "Basics",
      "Model Checking"
    ]
  },
  {
    "objectID": "ex04_logit-occs.html",
    "href": "ex04_logit-occs.html",
    "title": "Do you believe in ghosts?",
    "section": "",
    "text": "Ok lets try to imagine Port A is now facing a ghost epidemic - what’s the probability that your in a haunted house? Well we can use house hauntings as a binary outcome and model them as a function of some predictor. Likely, the most important thing influencing ghosts is the number of murders which have occurred in that house - so let’s use that as our predictor.\n\nPredicting Ghost Probability\n\nSimulating GhostsModelStanAnalysis\n\n\nWell this is probably an overesimate, but I’m simulating murders per house from a poisson distribution with a mean rate of 2/house. Let’s say it’s not Port A, but a particularly spooky town.\n\nset.seed(1031)\nlibrary(ggplot2) |&gt; suppressMessages()\nlibrary(bayesplot) |&gt; suppressMessages()\nlibrary(rstan) |&gt; suppressMessages()\nlibrary(ggpubr) |&gt; suppressMessages()\n\nrm(list = ls())\nsource('../R/utils.R') |&gt; suppressMessages() #note path\n\n\n###\n# Simulating Ghost probabilities ######\n###\n\nn_houses &lt;- 200 #number of houses to haunt\nn_murders &lt;- rpois(n_houses, 2) # mean 2 murders per house\n\nbeta_0 &lt;- -5\nbeta_1 &lt;- 2\n\np_ghost &lt;- 1 / (1 + exp(-(beta_0 + beta_1 * n_murders)))\n\n# Simulate presence/absence data\nhaunted &lt;- rbinom(n_houses, size = 1, prob = p_ghost)\n\nggplot() +\n  geom_point(aes(x = n_murders, y = haunted)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe above plot shows us the number of murders per house predicting if a house is haunted (1) or not (0).\n\n\nThe event of a house being haunted is a binary outcome \\(y_i\\). The probability it is haunted can be defined as \\(\\psi\\). We then can use a logistic regression to predict the probability for each house, where \\(m_i\\) is the number of murders in house \\(i\\)\n$$\n\\[\\begin{align}\n\ny_i &\\sim bern(\\psi_i)\\\\\n\nlogit(\\psi_i) &= \\beta_0 + \\beta_1 * m_i\\\\\n\\\\\n\\beta_0 &\\sim \\mathcal{N}(0, 5)\\\\\n\\beta_1 &\\sim \\mathcal{N}(0,2)\n\n\\end{align}\\]\n$$\n\n\n\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0, upper=1&gt; y;\n  vector[N] x;\n}\n\nparameters {\n  real beta_0;\n  real beta_1;\n}\n\nmodel {\n  // Priors\n  beta_0 ~ normal(0, 5);\n  beta_1 ~ normal(0, 2);\n  \n  // Likelihood\n  y ~ bernoulli_logit(beta_0 + beta_1 * x);\n}\n\n\n\n\n \\- Prep for Stan -----------\n\nghost_data &lt;- list(\n  N = n_houses,\n  y = haunted,\n  x = n_murders\n)\n\nghost_fit &lt;- stan(\n  './stan/ex04_simple-ghosts.stan',\n  'gfit',\n  data = ghost_data,\n  iter = 5000, warmup = 1000, cores =10\n)\nghost_fit\n\n\n\nInference for Stan model: gfit.\n4 chains, each with iter=5000; warmup=1000; thin=1; \npost-warmup draws per chain=4000, total post-warmup draws=16000.\n\n         mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nbeta_0  -5.36    0.01 0.71  -6.83  -5.82  -5.33  -4.86  -4.06  3211    1\nbeta_1   2.13    0.01 0.30   1.59   1.92   2.12   2.32   2.75  3143    1\nlp__   -66.02    0.02 1.00 -68.68 -66.42 -65.71 -65.30 -65.04  3982    1\n\nSamples were drawn using NUTS(diag_e) at Thu Oct 31 11:06:20 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nNow we can predict the probability a house is haunted based and compare it to our observed data:\n\nghost_post &lt;- extract(ghost_fit)\n\n# \\- Plot results ------------------\npred_range &lt;- seq(min(n_murders), max(n_murders), length.out = 1000)\n\n\nprob_matrix &lt;- matrix(NA, nrow = length(ghost_post$beta_0), ncol = length(pred_range))\nfor(i in 1:length(pred_range)) {\n  prob_matrix[,i] &lt;- plogis(\n    ghost_post$beta_0 + ghost_post$beta_1 * pred_range[i]\n  )\n}\n\npred_rel &lt;- data.frame(\n  mean = apply(prob_matrix, 2, mean),\n  low = apply(prob_matrix, 2, function(x) HDInterval::hdi(x)[1]),\n  high = apply(prob_matrix, 2, function(x) HDInterval::hdi(x)[2])\n)\n\n\nggplot() +\n  geom_point(aes(x = n_murders, y = haunted)) +\n  geom_line(\n    aes(x = pred_range, y = pred_rel$mean)\n  )+\n  geom_ribbon(\n    aes(\n      x = pred_range,\n      ymin = pred_rel$low,\n      ymax = pred_rel$high\n    ),\n    fill = 'lightblue',\n    alpha = 0.75\n  )+\n  labs(x = \"Number of Murders\", y = \"Pr(Ghost)\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if you just can’t feeeel them!\nThe previous example, once again is pretty unrealistic. Just because a ghost is there, you probably wouldn’t see them. Ghosts by definition are pretty ellusive.\nSo there’s 200 houses - and let’s say that they are all worried there’s a ghost in them. So they hire mediums to come a check it out. However, mediums may also struggle to identify a house - some houses are more clairvoyant (I googled this term) which means they are easier to connect to the spirit world in this setting. The paranomal researchers might take issue with the belief that people are clairvoyant not houses, but I’m setting the clairvoyance at the location. It’s all made up anyways.\nSo even if a ghost is there, it may not be detected. Now some houses were able to afford multiple medium visits to try and contact a ghost but not all. Let’s model this whole system.\n\nData SimulationModelStanAnalysisFurther Thoughts\n\n\nI’ve created a clvoy variable to be clairvoyance, which is pretty variable across houses. Then I calculate the true occupancy probability based on number of murders, we randomly simulate the number of visits per house. Each house has a probability of seeing a ghost, which is a function of its clairvoyance.\n\n#clairvoyance of house\nclvoy &lt;- rnorm(n_houses, mean = 10, sd = 4)\n\n\noccupancy_prob &lt;- plogis(beta_0 + beta_1 * n_murders) \n\n# Simulate occupancy states for each site\noccupancy &lt;- rbinom(n_houses, size = 1, prob = occupancy_prob)\n\n\nalpha_0 = -5\nalpha_1 = 0.3\n\ndetection_prob &lt;- plogis(alpha_0 + alpha_1 * clvoy)\n\nn_visits &lt;- sample(1:3, n_houses, replace = T)\n\ndetected &lt;- matrix(NA, n_houses, 3)\n\nfor (i in 1:n_houses) {\n  for (j in 1:n_visits[i]) {\n    if (occupancy[i] == 1) {\n      detected[i, j] &lt;- rbinom(1, size = 1, prob = detection_prob[i])\n    } else {\n      detected[i, j] &lt;- 0\n    }\n  }\n}\n\nggplot() +\n  geom_point(\n    aes(x = n_murders,\n    y = apply(detected, 1, function(x) min(1, sum(x,na.rm = T)))\n    )\n  )+\n  labs(x = 'Murders', y = 'Ghost Sightings') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLooking just at the raw data, we can tell it would be hard to extract a true relationship betweeen our variables. But we can model the two processes to tease apart this relationship\n\n\nFor fun, I set a really stronger prior indicating a lack of belief in ghosts in the \\(\\beta_0\\) prior.\n$$ \\[\\begin{align}\ny_{i} &= \\Sigma_j y_{ij}\\\\ &\\sim \\ \\left\\{\n  \\begin{array}{l}\n    0, \\ z_i = 0\\\\\n    bin(J_i, p_i), \\ z_i =1\\\\\n  \\end{array} \\right. \\\\\nz_i &\\sim bern(\\psi_i)\\\\\n\\\\\nlogit(p_i) &\\sim \\alpha_0 + \\alpha_1 * c_i\\\\\nlogit(\\psi_i) &\\sim \\beta_0 + \\beta_1 * m_i\n\\\\\n\\beta_0 &\\sim \\mathcal{N}(-10, 2)\\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 2)\\\\\n\\alpha_0 &\\sim \\mathcal{N}(0, 10)\\\\\n\\alpha_1 &\\sim \\mathcal{N}(0, 2)\\\\\n\n\\end{align}\\] $$\n\n\n\ndata {\n    int&lt;lower=0&gt; N; //houses\n    array[N] int&lt;lower=0&gt; v; // visits\n    array[N] int&lt;lower=0&gt; detections;\n    vector[N] x;\n    vector[N] c;\n}\n\nparameters {\n    real beta_0;\n    real beta_1;\n    real alpha_0;\n    real alpha_1;\n}\n\ntransformed parameters {\n    array[N] real p;\n    array[N] real psi;\n\n    for(i in 1:N) {\n        p[i] = inv_logit(alpha_0 + alpha_1 * c[i]);\n        psi[i] = inv_logit(beta_0 + beta_1 * x[i]);\n    }\n}\n\nmodel {\n    // Priors\n    beta_0 ~ normal(-10, 3);\n    beta_1 ~ normal(0, 2);\n    alpha_0 ~ normal(0, 10);\n    alpha_1 ~ normal(0, 2);\n\n    for (i in 1:N) {\n        detections[i] ~ binomial(v[i], p[i] * psi[i]);\n    }\n}\n\n\n\nOne thing worth noting here, I still don’t scale my data. If you are fitting multiple predictors, with different scales, scaling is a good idea. However, since the predictors are for different sub-models it is ok.\n\ndetection_data &lt;- list(\n  N = n_houses,\n  v = n_visits,\n  occupancy = occupancy,\n  detections = apply(detected, 1, sum, na.rm = T),\n  x = n_murders,\n  c = clvoy\n)\n\ndetection_fit &lt;- stan(\n  './stan/ex04_occupancy-ghosts.stan',\n  'occ-mod',\n  data = detection_data,\n  iter = 4000, warmup = 1000, cores = 10,\n  thin = 10 # it was slow since it estimates every individual p/psi\n)\ndetection_fit\n\n\n\n              mean    se_mean        sd        2.5%        25%        50%\nbeta_0  -7.6953879 0.05075728 1.8026893 -11.7039500 -8.8551699 -7.5896869\nbeta_1   3.1472114 0.02675336 0.9632905   1.5645386  2.4133122  3.0520817\nalpha_0 -6.9248825 0.04308833 1.4479416 -10.1836230 -7.7824426 -6.8042415\nalpha_1  0.4718402 0.00356620 0.1206168   0.2757358  0.3877794  0.4618578\n              75%      97.5%    n_eff      Rhat\nbeta_0  -6.335204 -4.7418411 1261.377 1.0009902\nbeta_1   3.786112  5.1559956 1296.456 1.0006136\nalpha_0 -5.901858 -4.4662894 1129.231 0.9994013\nalpha_1  0.541597  0.7399411 1143.942 0.9992079\n\n\nGreat! now we can plot our esimated posterior relationship of murders to ghosts, having accounted for detection probability\n\ndetect_post &lt;- extract(\n  detection_fit,\n  pars = c('beta_0', 'beta_1', 'alpha_0', 'alpha_1')\n)\n\n\n# \\- Plot results ------------------\npred_range &lt;- seq(min(n_murders), max(n_murders), length.out = 1000)\n\n\nocc_matrix &lt;- matrix(NA, nrow = length(detect_post$beta_0), ncol = length(pred_range))\nfor(i in 1:length(pred_range)) {\n  occ_matrix[,i] &lt;- plogis(\n    detect_post$beta_0 + detect_post$beta_1 * pred_range[i]\n  )\n}\n\nocc_rel &lt;- data.frame(\n  mean = apply(occ_matrix, 2, mean),\n  low = apply(occ_matrix, 2, function(x) HDInterval::hdi(x)[1]),\n  high = apply(occ_matrix, 2, function(x) HDInterval::hdi(x)[2])\n)\n\n\nggplot() +\n  geom_point(\n    aes(\n      x = n_murders,\n      y = haunted\n    ),\n    position = position_jitter(0,0.03),\n    alpha = 0.3,\n    color = 'red'\n  )+\n  geom_point(\n    aes(\n      x = n_murders,\n      y = apply(detected, 1, function(x) min(1, sum(x,na.rm = T)))\n    ),\n    position = position_jitter(0,0.01),\n    alpha = 0.3\n  ) +\n  geom_line(\n    aes(x = pred_range, y = occ_rel$mean)\n  )+\n  geom_ribbon(\n    aes(\n      x = pred_range,\n      ymin = occ_rel$low,\n      ymax = occ_rel$high\n    ),\n    fill = 'lightblue',\n    alpha = 0.75\n  )+\n    labs(x = 'Murders', y = 'Pr(Ghost)') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn the above figure, I overlayed real ghosts as red points and the observations at black points. We can see our model does a good job capturing the true relationship here!\nWe also can look at the effect of clairvoyance on ghost detection.\n\n# \\- Also look at the influence of clvoy --------\n\ncrange &lt;- seq(min(clvoy), max(clvoy), length.out = 1000)\n\n\ndetect_mat &lt;- matrix(NA, nrow = length(detect_post$alpha_0), ncol = length(crange))\nfor(i in 1:length(crange)) {\n  detect_mat[,i] &lt;- plogis(\n    detect_post$alpha_0 + detect_post$alpha_1 * crange[i]\n  )\n}\n\ndet_rel &lt;- data.frame(\n  mean = apply(detect_mat, 2, mean),\n  low = apply(detect_mat, 2, function(x) HDInterval::hdi(x)[1]),\n  high = apply(detect_mat, 2, function(x) HDInterval::hdi(x)[2])\n)\n\n\nggplot() +\n  geom_point(\n    aes(\n      x = clvoy,\n      y = apply(detected, 1, function(x) min(1, sum(x,na.rm = T)))\n    ),\n    position = position_jitter(0,0.01),\n    alpha = 0.3\n  ) +\n  geom_line(\n    aes(x = crange, y = det_rel$mean)\n  )+\n  geom_ribbon(\n    aes(\n      x = crange,\n      ymin = det_rel$low,\n      ymax = det_rel$high\n    ),\n    fill = 'lightblue',\n    alpha = 0.75\n  )+\n    labs(x = \"Clairvoyance\", y = 'Pr(Detection)')+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThis was a pretty basic case of an occupancy model, but it shows how we can use Bayesian inference to model different aspects of a system AND fill in data for points where we may have less observations. This can be expanded to have spatial compontent for unobserved areas or even include false positives (great for eDNA!)",
    "crumbs": [
      "Spooky Stuff!",
      "Do you believe in ghosts?"
    ]
  },
  {
    "objectID": "ex06_count-data.html",
    "href": "ex06_count-data.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Let’s count how many bigfoot observations there are across the country. For this we can turn to a great source of citizen science data - The Bigfoot Field Organization’s Geographic Database of Bigfoot / Sasquatch Sightings & Reports\n\nExploratory Data Analysis\n\n\nPoisson Regression\n\n\nZero-Inflated\n\n\nModelling Bigfoot Distribution"
  }
]