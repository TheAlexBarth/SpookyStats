[
  {
    "objectID": "ex01_single-parameter.html",
    "href": "ex01_single-parameter.html",
    "title": "Single Parameter Estimation",
    "section": "",
    "text": "Zombie bites - Estimating a binomial probability\nUh oh! You just got bit by a zombie. What’s the probability that you will now become a zombie too? Everyone knows that zombies are pretty infectious but maybe you were able to skirt by…\nLet’s simuate data for a vector of zombie bites. Let’s say that if a zombie bites you, there’s a 75% probability you’d get infected. We can simulate 25 events of zombie bites to observe our data from:\n\nData SimulationModel SpecificationStan CodeAnalysisFrequentist Comparison\n\n\n\n###\n# Single parameter examples\n###\nset.seed(1031)\nlibrary(ggplot2)\nlibrary(bayesplot)\nlibrary(rstan)\n\n###\n# First for a binomial value ######\n###\n\n\n# \\- simulate the data ---------\np = 0.75\nnum_cases = 50\nbites = rbinom(num_cases,1,p)\n\nIt’s worth noting that our simulated data shows a different simulated value than our true probability - which will influence our posterior estimates!\n\n\nWe have to specify our model to be able to construct it into stan code. We are going to use a bernoulli distribution as our data model (we think that zombie infection following a bite will occur with some probability (\\(\\theta\\)) according to a bernoulli distirubiton). Note this also could be done with the binomial distribution…\nWe have to then model our prior belief about the infection probability. Since we simluated the data, we actually do know the true parameter value, but we pretend to learn. So let’s say you are pretty optimisitic and think it may only be a 50/50 chance that infection occurs after a bite, but you’re not too sure. We can then model our prior distribution as a beta(2,2), which is loosely centered around 0.5, but not too strong in it.\nSo our model looks like this:\n\\[\n\\begin{align}\nData \\ Model \\ &\\{ \\ y_i \\sim bernoulli(\\theta) \\\\\nParameter \\ Model \\ &\\{ \\theta \\sim beta(2,2)\n\\end{align}\n\\]\n\n\n\n// This is saved as a .stan file\ndata {\n  int&lt;lower=0&gt; n;         // Number of trials\n  array[n] int&lt;lower=0, upper=1&gt; y; // Observed outcomes (0 or 1)\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p; // Probability of success\n}\n\nmodel {\n    // Prior:\n    p ~ beta(2, 2);\n    \n    y ~ bernoulli(p);\n}\n\n\n\nWe call the stan code in R to run the model:\n\n# \\- Set up data for stan\n\nbite_data &lt;- list(\n  n = num_cases,\n  y = bites\n)\n\nzombie_fit &lt;- stan('./stan/ex01_single-param.stan', 'zombie-mod', data = bite_data, iter = 3000, warmup = 1000)\n\n\n\nInference for Stan model: zombie-mod.\n4 chains, each with iter=3000; warmup=1000; thin=1; \npost-warmup draws per chain=2000, total post-warmup draws=8000.\n\n       mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\np      0.66    0.00 0.07   0.52   0.61   0.66   0.70   0.78  2481    1\nlp__ -33.32    0.01 0.73 -35.33 -33.49 -33.03 -32.85 -32.80  3864    1\n\nSamples were drawn using NUTS(diag_e) at Mon Oct 28 19:58:08 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe text which follows our model output gives us a lot of useful information both to the posterior values and model convergence diagnostics.\nWe can also visualize the distribution estimated by the posterior value. I’ve added a vertical red line for the true value, but in most real cases, you don’t know the “true” value.\n\n# get posterior for the estimates of p\nzombie_p_post &lt;- rstan::extract(zombie_fit)$p\n\nposterior_param_plot(zombie_p_post, dbeta, seq(0,1,length.out = 1000), p, shape1 = 2, shape2 = 2)\n\n\n\n\n\n\n\n\nThe posterior high-density interval captures the true value. However, it is worth noting that our data was simulated, and did not reflect the true value that closely (0.66 versus 0.75)\n\n\nFor fun, I’ll add a frequentist comparison:\n\nprop.test(x = sum(bites), n = length(bites))\n\n\n    1-sample proportions test with continuity correction\n\ndata:  sum(bites) out of length(bites), null probability 0.5\nX-squared = 4.5, df = 1, p-value = 0.03389\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5114459 0.7840536\nsample estimates:\n   p \n0.66 \n\n\nWe could use this approach to say: “we are 95% confident that the true value is between 0.51 and 0.78.”. Or you could talk about the p-value and say our observed propotion of bite cases is significantly different from 0.5.\nSo in this case, it is not really that different than our Bayesian analysis, just a philosophical difference in the way we interpret the underlying process.\n\n\n\n\n\nWerewolf weights - Estimating a mean value\nI’ve googled around and found that werewolfs are up to 180kg. Let’s simulate some data to reflect this.\n\nData Simulation\n\n\nGiven the fact they are up to 180kg, let’s assume they are average 140 with a standard deviation of 15kg - a pretty big range!\n\n\n\n\n\nYeti vs Bigfoot - comparing two groups\nLet’s say we had a debate about who was bigger - Yeti or Bigfoot.\nAfter googling and reading some strange websites, I’ve found that Well, I can simulate some data to",
    "crumbs": [
      "Spooky Stuff!",
      "Single Parameter Estimation"
    ]
  },
  {
    "objectID": "03_Model-Checking.html",
    "href": "03_Model-Checking.html",
    "title": "Model Checking",
    "section": "",
    "text": "The process of model checking for a Bayesian analysis involves two steps: seeing if the estimation of the posterior was good, then seeing if the model fit is actually useful. The assumptions of a Bayesian analysis are slightly more relaxed compared to traditional frequentist analyses but it requires a bit more thought when constructing and evaluating the model. You should be able to justify your subjective decisions in model building.\n\n\nThe first thing to consider when fitting a Bayesian model is if your MC Chains are well mixed, or converged. This means that you are doing a good job estimating the posterior as the chains have explored, and largely landed, in simimlar regions of high density for values of the posterior. There are several diagnostic tools to check for model convergence, many of which are automatically generated in output from stan or easily accessible using functions in r (check out bayesplot).\nSome things which are worth considering:\n\nRhat: this is a metric evaluating how well the chains converged. It should be near 1 (stan says acceptable around &lt;1.1) but this can be context dependent.\nESS: Effective sample size is a metric which measures how many samples can be considered independent from your chains. While I didn’t go into detail, since MCMC estimation of the posterior relies on sequentially adding proposal values for \\(\\theta\\), those values are inherently autocorrelated so ESS calculates a value for how many can be considered independent while accounting for an autocorrelation factor (Kruschke equation 7.11).\nTraceplots: you can visualize the MCMC chains using things like bayesplot::mcmc_trace(fit) to see if the chains are well mixed. This also is a good oppoturnity to see if there was the propper amount of burn-in.\n\n\n\n\nOnce your confirmed if the chains are well mixed, you can see if the posterior distribution and overall model structure are actually useful for your data. This can be done a number of ways:\n\nPPC: Posterior Predictive Checks involve using the model to simulate what is expected with the estimation of the posterior distribution. Comparing these estimates allows us to see how well the predictions match the observed data. This is fundamentaly just like checking residuals and you can even do all the residual plots which you might be familiar with. This is easily done using the bayesplot package in r with pp_check()\nYou can check residuals using common residual tests or things like calculating RMSE.\nBayes P-value: A bayes p-value checks if your predicted data match the observed data using some discrepancy metric\nCross-validation: This is an approach more common in ML applications but works really well here. Esspecially applicable if you are interseted in using your posterior distributions for prediction or forecasting. These procedures involve selectively leaving out some of your data, then evaluating the model’s ability to predict that data (leave-one-out or k-fold are common approaches).",
    "crumbs": [
      "Basics",
      "Model Checking"
    ]
  },
  {
    "objectID": "03_Model-Checking.html#convergence-diagonsitics",
    "href": "03_Model-Checking.html#convergence-diagonsitics",
    "title": "Model Checking",
    "section": "",
    "text": "The first thing to consider when fitting a Bayesian model is if your MC Chains are well mixed, or converged. This means that you are doing a good job estimating the posterior as the chains have explored, and largely landed, in simimlar regions of high density for values of the posterior. There are several diagnostic tools to check for model convergence, many of which are automatically generated in output from stan or easily accessible using functions in r (check out bayesplot).\nSome things which are worth considering:\n\nRhat: this is a metric evaluating how well the chains converged. It should be near 1 (stan says acceptable around &lt;1.1) but this can be context dependent.\nESS: Effective sample size is a metric which measures how many samples can be considered independent from your chains. While I didn’t go into detail, since MCMC estimation of the posterior relies on sequentially adding proposal values for \\(\\theta\\), those values are inherently autocorrelated so ESS calculates a value for how many can be considered independent while accounting for an autocorrelation factor (Kruschke equation 7.11).\nTraceplots: you can visualize the MCMC chains using things like bayesplot::mcmc_trace(fit) to see if the chains are well mixed. This also is a good oppoturnity to see if there was the propper amount of burn-in.",
    "crumbs": [
      "Basics",
      "Model Checking"
    ]
  },
  {
    "objectID": "03_Model-Checking.html#evaluating-model-fit",
    "href": "03_Model-Checking.html#evaluating-model-fit",
    "title": "Model Checking",
    "section": "",
    "text": "Once your confirmed if the chains are well mixed, you can see if the posterior distribution and overall model structure are actually useful for your data. This can be done a number of ways:\n\nPPC: Posterior Predictive Checks involve using the model to simulate what is expected with the estimation of the posterior distribution. Comparing these estimates allows us to see how well the predictions match the observed data. This is fundamentaly just like checking residuals and you can even do all the residual plots which you might be familiar with. This is easily done using the bayesplot package in r with pp_check()\nYou can check residuals using common residual tests or things like calculating RMSE.\nBayes P-value: A bayes p-value checks if your predicted data match the observed data using some discrepancy metric\nCross-validation: This is an approach more common in ML applications but works really well here. Esspecially applicable if you are interseted in using your posterior distributions for prediction or forecasting. These procedures involve selectively leaving out some of your data, then evaluating the model’s ability to predict that data (leave-one-out or k-fold are common approaches).",
    "crumbs": [
      "Basics",
      "Model Checking"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spooky Stats: A brief introduction to Bayesian modelling in R",
    "section": "",
    "text": "What this is:\nI put together this page as a set of examples of why Bayesian inference can be useful or exciting. I’m assuming users have some familiarity with statistics or probability, likely from at least one undergrad level course. I’m also assuming you have some familiarity with working in R as that is what I use for all these examples.\nThese examples should offer a starting point or motivation for constructing your own analyses. I constucted these examples from an ecology/marine science mindset. However, because I wrote these examples to share on Halloween, they are extra spooky. So, these examples are hopefully thought-provoking for both ecologists, marine scienstists and paranormal researchers or cryptozoologists.\n\nReplicating these examples:\nThis website is hosted from a github repo. You can copy and paste code from each article to construct your own analyses. However, you can also access all the scripts - not in a notebook format - directly from the github repo. If you clone the repo, you can run the examples from each script and it will nicely integrate with all the data I pull in some of the more complicated examples.\n\n\n\nWhat this is not:\nThis is not a comprehensive introcution, or even close to it. I constructed this to be a short introduction as a motivation or starting point for your own investigation. While the code in these examples may be a useful reference, it does not contain all the information you will need to conduct, and importantly - check, your own analyses. I largely don’t cover the concepts behind MCMC, probability distributions, or the underlying mechanics for Bayesian inference. You definitely should explore more as there are resources better than I could write which can guide you.\n\nAlso, while in the provided examples, I do share some methods for reporting results of Bayesian analyses, it is far from a comprehensive guide of what should be shared.\nTake a look at the additional resources\nAlso, this is not an argument for the existence of ghosts, werewolves, or zombies - just attempts to update our beliefs on their probabilities.\n\n\nSlides:\nHere are the slides from my first presentation of these examples.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "01_quick_bayes_introduction.html",
    "href": "01_quick_bayes_introduction.html",
    "title": "Probably Useful - Quick comments about probability",
    "section": "",
    "text": "Probability\nMany ecologists learn some basic statistics in undergrad, with maybe some advanced training in grad school. However, we mostly learn through other papers or patchwork approaches while trying to analyze our own data. As a consequence, there isn’t always a formal introduction to basic concepts of probability. However it is important for us consider some of the fundamental ideas underneath this concept of “probability” which we are using when we conduct our analyses.\nSimilar to the words “hypothesis” and “theory”, words like “probability”, “chance”, “odds” are common to use in english but the connotation of these words is not often considered. When talking about statistics, the term “probability” is quite a loaded word and may be interpretted in a number of ways. Bulmer (1965)’s first chapter offers a nice discussion on the theoretical implications of two concepts of probability, which he defines as “statistical probability” and “inductive probability”. Similarly, Kruschke (p73-77) discusses these two terms as “outside the head” and “inside the head” respectively (possibly a more approachable read given modern writing). Hopefully not to complicate it further, I’m going to use the terms “frequency” and “credibility” to breifly discuss these two different probability concepts.\n\nFrequency Definition\n\nThis is the more formal definition of probability which is likely introduced in a class on probability. It is the idea that probability is a measure of a long-run relative frequency of some specified event. The classic example is that the probability of getting heads on a fair coin flip is 0.5, which can be demonstrated by flipping a coin some large number of times and calculating the proportion of head. Authors will often refer to this as an empirical approach to probability as it can be demonstrated (hence Kruschke’s “outside the head” term as it can occur in the real world).\nWe can simulate such a probability to see as our sample size increases, the measured frequence of heads converges to the fixed true probability value:\n\nset.seed(1031)\nlibrary(ggplot2)\n\ntrue_prob = 0.5\nnum_trials = seq(1,1000,1)\ncoin_result = sapply(num_trials, function(x) rbinom(1, x, true_prob)) / num_trials\n\nggplot() +\n   geom_line(aes(x = num_trials, y = coin_result)) +\n   labs(x = 'Number of Coin Flips', y = 'Frequency of Heads') +\n   theme_minimal()\n\n\n\n\n\n\n\n\nClearly, increasing the number of trials allows us to converge towards the true probability.\n\nCredibility Definition\n\nAlternative to the frequency interpretation of probability, we have a more colloquial use of probability. This is like “what’s the probability Texas makes it to the SEC championship game” or “What’s the probability the ghosts of all the fish we’ve caught will haunt us forever?”\nThis use of the term probability is more referring to the credibility of some event or the strength of belief in a statement. This terminology, while more intuitive from a thinking standpoint, is less straightforward to define in a mathematical concept. Thus, many approaches of orthodox statistics try to hammer us away from using this terminology of probability.\nTo quote the end of Bulmer’s 1965 introduction to Principles of statistics:\n\nSuch a psychological scale varying from one person to another is of little use in scientific discussion in which the scientist mush persuade others to believe what he himself believes.\nIt has been reluctantly concluded by most statisticians that inductive probability cannot in general be measured and, therefore, cannot be used in the mathematical theory of statistics… …there seems no reason why rational degrees of belief should be measurable any more than, say, degrees of beauty… it does not seem possible to construct a numerical scale of such (inductive) probabilities.\n\nThis is quite the argument against the credibility definition of probability. However, the basic idea of Bayesian analysis does exactly what Bulmer describes as impossible: assign mathematical measurements to characterize levels of belief (or credibility) to some event or value. Both Gelman (2013) and Kruschke (2015) offer criticisms with the frequency definition - and how it is not as objective as the frequentists claim. I don’t go into those arguments here but to quote Gelman Chapter 1:\n\nIn Bayesian statistics, probability is used as the fundamental measure or yardstick of uncertainty. Within this paradigm, it is equally legitimate to discuss the probability of ‘rain tomorrow’ or of a Brazilian victory in the soccer Wold Cups as it is to discuss the probability that a coin toss will land heads…\n… Bayesian methods enable statements to be made about the partial knowledge available (based on data) concerning some situation or ‘state of nature’ (unobservable or as yet unobserved) in a systematic way, using probability as the yardstick. The guiding principle is that the state of knowledge about anything unknown is described by a probability distribution.\n\n\n\nNotation & Definitions:\n\nProbability distributions & random variables\nWhen describing a model, we will use notation to define which distributions we are using to model our parameters and data-generating processes. So it is important to be familiar with these things. A probability distribution, to quote Kruschke, is “simply a list of all possible outcomes and their corresponding probabilities”. Effectively, it is a function used to describe the distribution of probabilities around some event. There are several probability density (or mass for discrete data) functions which can be used to describe distributions. For example, the binomial distribution would be used to define the coin-toss example. Alternatively, a normal distribution may be used to describe the weight of werewolves from some population.\nA variable is said to be a random variable if it is value is generated by a stochastic process (sampled by a probability distribution). For example, if werewolf weight, \\(W\\), can be described by a normal distribution with mean, \\(\\mu\\), and variance, \\(\\sigma^2\\), we can write \\(W \\sim \\mathcal{N}(\\mu, \\sigma^2)\\). Probabilitistic statements can then be written as \\(Pr(W = w) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{1}{2}(\\frac{w-\\mu}{\\sigma})^2)\\) to say “the probability any given werewolf weighs \\(w\\) is equal to the probability density function (PDF) for a normal distribution evaluated at \\(w\\)”.\nIn bayesian analyses we end up writing a lot of distributions and notation. Since it is not always easy to write out the distributions specified, or because we may not be referring to a defined distribution, we need a short hand for specifying the PDF. Some authors (Gelman, Kruschke) use the notation \\(p(\\cdot)\\). However, this becomes cumbersome when writing out more detailed models or formulations. In most cases, ecologists may not encounter this as a huge issue (I’ve seen papers with \\(\\theta \\sim Dist(\\theta)\\) which is even messier). However, it is fairly common in Bayesian literature to use bracket notation, \\([\\cdot]\\) for a PDF.\nSo, in our example: \\[\n\\begin{split}\nW \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\equiv\nW \\sim p(W)\n\\equiv\nW \\sim [W]\n\\end{split}\n\\]\nNote in this example, I used a normal distribution but \\([\\cdot]\\) can be any distribution in practice.\n\n\nConditional Probability\nFull probability theory and introduction is way outside the scope of what’s intended here. But it is important to define the notation of conditional probability. We use the notation \\([a | b]\\) to say the probability of \\(a\\) conditional on \\(b\\). In other words if we know \\(b\\), what is the probability of \\(a\\)?\nThe formula of conditional probability can be defined as:\n\\[\n[a|b] = \\frac{[a, b]}{[b]}\n\\]\nThe joint distribution of \\([a,b]\\) is the product probability of both \\(a\\) and \\(b\\) conditioned on \\(a\\):\n\\[\n[a,b] = [a][b|a]\n\\]\nsee Hooten & Hefley chapter 1 for all details\n\n\nBayes Rule:\nNow that we have some basics, we can define the Bayes rule which underlies this whole idea:\nUsing the conditional distribution \\([a|b]\\), we can substitute in the definition of \\([a,b]\\) \\[\n\\begin{align}\n[a|b] &= \\frac{[a, b]}{[b]}\\\\\n\\\\\n&= \\frac{[a][b|a]}{[b]}\n\\end{align}\n\\]\nThis is the fundamental concept behind Bayesian analysis which I’ll discuss below. For a fun bit of history, Bayes rule was named after Reverend Bayes but Bayes did not publish his own work and left the implications for it somewhat unfinished. It was Richard Price who discovered his work and published it in 1763. However, it is worth noting that really it was Laplace who independently developed Bayes’s rule and published it in 1774 with more direct applications. Despite the early definitions of Bayes rule, which as you’ll see allows its application to mathematically formulate inductive probabilities, it was not widely adopted. Most of statistics throughout the 20th century was descended from Fisher’s work on frequentist analyses - which places Bulmer’s comments into much clearer context. As you’ll see, Bayesian analysis, while not exclusively, typically typically rely stochastic simulations which require strong computational resources. So it hasn’t been until the past couple of decades Bayesian analyses are approachable for modern cases. As computation resources increase, the power of bayes only grows.\n\n\n\nApplications to analyses\nNow with a basic grasp of the fundamental pieces behind probability - let’s discuss briefly how this all works its way into statistical inference.\n\nFrequentist statistics\nIf you’ve run any analyses on your data there is a good chance you’ve used what are referred to as frequentist statistics. These are essentially anything which rely on a p-value to interpret results. These analyses all rely on the long-run frequency interpretation of probability\nExplaining all of frequentist statistics is a long winded In short the idea regarding frequentist analysis is that we are interested in some population level parameter which has a true, fixed value. We then collect a sample from this population and using our sample statistics, we can use some inferential statistical analyses (or tests) to try and make inferences about that parameter. In past classes, I used this diagram to describe this process:\n\nWhen statistically testing our samples for population level inference, we are saying, “if I collected this sample many, many repeated times what would my sample look like?”. Thus most tests will report both a confidence interval and p-value. First, let’s talk confidence intervals. Usually a stats course might teach students this means, “I am 95% confident that the true population-level parameter is within this range.” Although, more technically the confidence interval could be thought in a frequentist context of “If the we sampled from the population distribution many, many times 95% of the time, our sample would capture the true parameter value”. I always found this concept very well characterized by an app made by the professor of my undergraduate probability course linked here\nA p-value is an extension of the confidence interval in some ways. What most students will learn, and use, is that a small p-value means that your findings are “statistically significant”. But what does that really mean? Generally, a p-value is a result from some test which in technical terms is the probability of a type-i error, or rejecting the idea that your parameter of interest is different from some other value (usually 0). In more approachable language - a small p-value suggests that your parameter is statistically significantly different than 0 (or some other value). Thinking in frequentist ideas, it is the probability you would draw that sample of data, if the true population parameter is 0. However it is in general sticky to explain and why many students (and scientists) do not correctly discuss their results.\nOften times, I think ecologists are not actually interested in the results of the frequentist statistical analyses which they are running. However, this is often all which is learned and there is the never-ending hunt for a “statistically significant” result which unfortunately drives us in the wrong direction. One thing, which is well explained in Kruschke’s chapter on NHST, is that the p-value really depends on sample size in most cases and can bias our conclusions (See figure 11.1).\nI think that these analyses do have their place but are often misunderstood and misused. These short sentences are far from a full discussion on the advantages and challenges of frequentist analyses but see some of these resources:\n\nKruschke chapter 11\nSullivan & Feinn 2012\nThis vox article\nDushoff et al. 2019 (I like this one a lot)\n\nAgain this was far from a complete introduction to frequentist stats, but here’s an undergraduate-level reference sheet I wrote a while ago: link\n\n\nBayesian analysis:\nSo what we’ve all waited for - Bayesian analysis. The fundamental idea of Bayesian analysis is that everything is treated as a random variable, meaning it is drawn from some distribution. So if we are interested in a population parameter - we don’t assume it has some true fixed value, but rather we are interested in describing the distribution of that parameter. We can use bayes rule to describe parameter distribution using both prior knowledge of that parameter, \\(\\theta\\), and observed data, \\(y\\).\nThis is better described to returning our Bayes rule,\n\\[\n[\\theta | y] = \\frac{[y | \\theta][\\theta]}{[y]}\n\\]\nThis effectively is saying what’s the probability of \\(\\theta\\), or a hypothesis, given \\(y\\), or data I’ve observed. So in words:\n\\[\n[Hypothesis | data] = \\frac{[data | Hypothesis][Hypothesis]}{[data]}\n\\]\nLet’s break down each part of this into words:\n\n\\([\\theta|y]\\) - the probability distribution of a parameter given data. This is called the posterior distribution\n\\([y| \\theta]\\) - the probability of that data given the parameter value. This is often called the likelihood of the data or in practice we will define some model, called a “data model” where we define some process which generates data under the parameters of interest.\n\\([\\theta]\\) - the probability of that parameter. This is referred to as the prior distribution. Here, we can incorporate prior knowledge or assumptions about the parameter\n\\([y]\\) - the probability of the data under the model. Also referred to as evidence or marginal likelihood\n\nThe last piece here is the most complicated as it can be tricky to calculate as it is the probability of \\(y\\) under all possible values of \\(\\theta\\). This means:\n\\[\n[y] = \\int [y|\\theta][\\theta]d\\theta\n\\]\nHowever, by integrating over all possible values of \\(\\theta\\), \\([y]\\) will be calculated as a constant. As such, it only serves to scale the numerator in bayes rule and is not actually necessary to describe the posterior distribution. So we can update our formula to say:\n\\[\n[\\theta | y] \\propto [y | \\theta][\\theta]\n\\]\nThus, our posterior distribution is proportional to the data model (likelihood) and the prior distribution. In practice, this means we can take our prior belief (\\([\\theta]\\)) and update it based on data. To quote Kruschke’s opening lines of Chapter 2: “Bayesian inference is reallocation of credibility across possibilities”. We can specify what we thought about something, then based on new information update our belief. Not only does this allow for the use of the inductive terminology of probability, it much more closely matches our intuitive way to discuss science. What do we think about a hypothesis before a study and how does this update when we finish the study. For a spooky example: Let’s say you firmly believe your house is haunted, then we bust out all the tools to look for ghosts and find absolutely nothing, your updated belief in the haunted house likely declined a bit.\nIn Bayesian data analyses we typically don’t concern ourselves with Null-hypothesis significance testing (p-values). Instead, we often will report credible intervals - a range of values which best captures the bulk of the posterior distribution. These can be 95% quantiles, however more often a high-density interval is used as it can capture non-uniform distributions.\nAll this will hopefully become clear in our examples upcoming:",
    "crumbs": [
      "Basics",
      "Probably Useful - Quick comments about probability"
    ]
  },
  {
    "objectID": "02_mcmc_tech.html",
    "href": "02_mcmc_tech.html",
    "title": "Describing the posterior",
    "section": "",
    "text": "Why?\nThe aim of Bayesian inference is to describe \\([\\theta | y]\\), our posterior distribution. Then we can describe things like the posterior mean and credible intervals. However, describing the posterior distribution is a challenge. Think about how we updated bayes rule to this proportionality statement: \\([\\theta | y] \\propto [y | \\theta][\\theta]\\).\nSo to calculate characteristics of the posterior, we’d need to be able to describe it. Since we are removed the demoninator (\\([y]\\)), it becomes difficult. In some limited cases, we can have the posterior match the form of the prior (conjugate) and we can define all cases for the marginal likelihood, allowing us to solve for the posterior analyticalt. But such cases are rare.\nAs a result, Bayesians use stochastic methods to approximate the posterior distribution\n\n\nMonte Carlo Methods\n\n\n\n\n\n\nWarning\n\n\n\nThe overall concepts behind MCMC methods or other stochastic approximation tools are far beyond the scope of what I’ve put together here. It is worth reading a little more detail elsewhere. Probably the most comprehensible, yet still accessible, introduction to MCMC is in Hooten & Hefley 2015. Gelman 2013 Chpater 10 provides a lot of detail and Kruschke Chapter 7 also provides a pretty useful introduction to Gibbs sampling with some general model-checking resources.\nHere, I try to summarize those works and provide a short, but useful start.\n\n\nThe posterior distribution can be approximated by generating a really large sample from it. To show this concretely take an example with a normal distribution. While we know the form of a normal distribution, we can approximate it by generating a large number of random samples from that distribution, this process is called Monte Carlo (MC) sampling.\n\nset.seed(1031)\nsample_size = 100\ndata = rnorm(sample_size)\nhist(data, probability = TRUE, main = \"Histogram with Normal Curve\")\ncurve(dnorm(x, mean = mean(data), sd = sd(data)), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nqqnorm(data)\nqqline(data, col = 'blue')\n\n\n\n\n\n\n\n\nWith only 100, it doesn’t do too good a job approximating the distribution but if you run the examples and increase to 1000, it’ll work a little better. (Even better with more!)\nWell this makes sense with a known distribution, we don’t know our posterior distribution. So what is done for Bayesian computation is generate a posterior using “guesses” for values of \\(\\theta\\). This can be denoted as \\(\\theta^* \\sim [\\theta]\\). For a bunch of guesses, we can generate a whole samples of \\(\\{\\theta^{(1)}, ..., \\theta^{(k)}\\}\\) for \\(k\\) guesses.\nSince we are interested in features of the parameter (like mean or high density intervals), we can define a generic function of the distribution as \\(\\mathcal{f}(\\theta)\\). So expectated values of the poseterior distribution would be \\(E(\\mathcal{f}(\\theta) | y)\\). With this in mind:\n\\[\n\\begin{align}\nE(\\mathcal{f}(\\theta) | y) &= \\int \\mathcal{f}(\\theta) [\\theta | y]d\\theta \\\\\n&\\approx \\sum_{k=1}^{k}{\\mathcal{f}(\\theta^{(k)})}\n\\end{align}\n\\] (equations 10.1 gelman and 3.1 hooten & helfley)\nThe vector of \\(\\theta^{(k)}\\) can be generated by MC sampling. Often this is Markov Chain MC (MCMC), although other methods exist, notably HMC (see below). Without going into detail, this process works by using some algorithm to generate suggestions for new values of \\(\\theta\\), which are added based on some criteria to the vector \\(\\theta^{(k)}\\). These algorithms will propose and accept values based on data and prior specifications of \\([\\theta]\\). This then is used to approximate our posterior distribuiton.\nAll MC algorithms will genereate the vector of \\(\\theta^{(k)}\\) for some user specified length. These vectors are often referred to as “chains”. You can think of the chain as working its way around the distribution, so the more time it has to explore the parameter space, the better. Chain length is an important consideration as it will improve estimates of the posterior to have longer chains, but this will increase computation cost. 2000 is a good minimum but this depends on the overall number of parameters.\nSimilarly, multiple chains are run (typically 3-4) and this allows multiple attempts at the posterior distribution. The chains can be compared for convergence to evaluate how well the model is working.\nSince the chain starts at some (usualy random) initial value, it needs time to work its way into the center of the high-density regions of the posterior distribution. So usually some amount of the beginning fo the chain is discarded, the amount of discarded MC samples is called burn-in\nFinally, sometimes chains are thinned, where samples are removed from the posterior at some rate. This typically is only done to reduce memory constraints. However many intro tutorials may make this a routine item, but it is not necessary and may even be detrimental.\nSo far, I’ve mostly provided examples in the case of a single-parameter, but as you may have guessed from the langauge and will see in this example, posterior distribution may have multiple, hierarchical parameters so it is estimating a joint posterior distribution across many dimensions - clearly not so simple!\n\n\nTechnology\nYou don’t have to know how to build an engine to be able to drive a car. And thankfully, you don’t have to write your own algorithms in order to conduct Bayesian analyses. There are several tools which play nicely with R to conduct your analyses. Here I use stan but there are several others. See the additional resources tab.",
    "crumbs": [
      "Basics",
      "Describing the posterior"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]