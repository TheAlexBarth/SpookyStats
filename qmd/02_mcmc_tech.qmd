---
title: Describing the posterior
---

# Why?

The aim of Bayesian inference is to describe $[\theta | y]$, our posterior distribution. Then we can describe things like the posterior mean and credible intervals. However, describing the posterior distribution is a challenge. Think about how we updated bayes rule to this proportionality statement: $[\theta | y] \propto [y | \theta][\theta]$.

So to calculate characteristics of the posterior, we'd need to be able to describe it. Since we are removed the demoninator ($[y]$), it becomes difficult. In some limited cases, we can have the posterior match the form of the prior (conjugate) and we can define all cases for the marginal likelihood, allowing us to solve for the posterior analyticalt. But such cases are rare.

As a result, Bayesians use stochastic methods to approximate the posterior distribution


# Monte Carlo Methods


:::{.callout-warning}
The overall concepts behind MCMC methods or other stochastic approximation tools are far beyond the scope of what I've put together here. It is worth reading a little more detail elsewhere. Probably the most comprehensible, yet still accessible, introduction to MCMC is in Hooten & Hefley 2015. Gelman 2013 Chpater 10 provides a lot of detail and Kruschke Chapter 7 also provides a pretty useful introduction to Gibbs sampling with some general model-checking resources.

Here, I try to summarize those works and provide a short, but useful start.
:::

The posterior distribution can be approximated by generating a really large sample from it. To show this concretely take an example with a normal distribution. While we know the form of a normal distribution, we can approximate it by generating a large number of random samples from that distribution, this process is called Monte Carlo (MC) sampling.

```{r}
set.seed(1031)
sample_size = 100
data = rnorm(sample_size)
hist(data, probability = TRUE, main = "Histogram with Normal Curve")
curve(dnorm(x, mean = mean(data), sd = sd(data)), add = TRUE, col = "blue", lwd = 2)
```

```{r}
qqnorm(data)
qqline(data, col = 'blue')
```

With only 100, it doesn't do too good a job approximating the distribution but if you run the examples and increase to 1000, it'll work a little better. (Even better with more!)


Well this makes sense with a known distribution, we don't know our posterior distribution. So what is done for Bayesian computation is generate a posterior using "guesses" for values of $\theta$. This can be denoted as $\theta^* \sim [\theta]$. For a bunch of guesses, we can generate a whole samples of $\{\theta^{(1)}, ..., \theta^{(k)}\}$ for $k$ guesses.

Since we are interested in features of the parameter (like mean or high density intervals), we can define a generic function of the distribution as $\mathcal{f}(\theta)$. So expectated values of the poseterior distribution would be $E(\mathcal{f}(\theta) | y)$. With this in mind:

$$
\begin{align}
E(\mathcal{f}(\theta) | y) &= \int \mathcal{f}(\theta) [\theta | y]d\theta \\
&\approx \sum_{k=1}^{k}{\mathcal{f}(\theta^{(k)})}
\end{align}
$$
(equations 10.1 gelman and 3.1 hooten & helfley)

The vector of $\theta^{(k)}$ can be generated by MC sampling. Often this is Markov Chain MC (MCMC), although other methods exist, notably HMC (see below). Without going into detail, this process works by using some algorithm to generate suggestions for new values of $\theta$, which are added based on some criteria to the vector $\theta^{(k)}$. These algorithms will propose and accept values based on data and prior specifications of $[\theta]$. This then is used to approximate our posterior distribuiton.

All MC algorithms will genereate the vector of $\theta^{(k)}$ for some user specified length. These vectors are often referred to as "***chains***". You can think of the chain as working its way around the distribution, so the more time it has to explore the parameter space, the better. Chain length is an important consideration as it will improve estimates of the posterior to have longer chains, but this will increase computation cost. 2000 is a good minimum but this depends on the overall number of parameters. 

Similarly, multiple chains are run (typically 3-4) and this allows multiple attempts at the posterior distribution. The chains can be compared for ***convergence*** to evaluate how well the model is working.

Since the chain starts at some (usualy random) initial value, it needs time to work its way into the center of the high-density regions of the posterior distribution. So usually some amount of the beginning fo the chain is discarded, the amount of discarded MC samples is called ***burn-in***

Finally, sometimes chains are ***thinned***, where samples are removed from the posterior at some rate. This typically is only done to reduce memory constraints. However many intro tutorials may make this a routine item, but it is not necessary and may even be [detrimental](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00131.x).

So far, I've mostly provided examples in the case of a single-parameter, but as you may have guessed from the langauge and will see in this example, posterior distribution may have multiple, hierarchical parameters so it is estimating a joint posterior distribution across many dimensions - clearly not so simple!

# Technology

You don't have to know how to build an engine to be able to drive a car. And thankfully, you don't have to write your own algorithms in order to conduct Bayesian analyses. There are several tools which play nicely with R to conduct your analyses. Here I use stan but there are several others. See the additional resources tab.